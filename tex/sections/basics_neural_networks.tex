We consider sequential neural networks as they are simple and widely used.
They are composed of $L$ layers $f^{(l)}(\cdot, \vtheta^{(l)}), l=1,\dots, L$, each of which can have its own parameters $\vtheta^{(l)}$.
The whole network is simply a stack of layers, \ie $f = f^{(L)} \circ \dots \circ f^{(1)}$, and the evaluation produces a set of intermediate features,
\begin{align*}
  \vx^{(l)} = f^{(l)}(\vx^{(l-1)}, \vtheta^{(l)}), \quad l=1,\dots, L,
\end{align*}
starting with $\vx^{(0)} \leftarrow \vx$, ending in $\vx^{(L)} \leftarrow f(\vx, \vtheta)$.
We refer to the hidden representations $\vx^{(l-1)}$ as the \emph{input to layer $l$}, and to $\vx^{(l)}$ as the \emph{output of layer $l$}.
Some layers have empty parameters, e.g.\,activation, pooling, or dropout layers.

To compute KFAC, we will need access to the inputs and outputs of certain layers.
We can obtain these by intercepting the neural net's forward pass using PyTorch's hook mechanism, see \Cref{basics/forward_pass}.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
