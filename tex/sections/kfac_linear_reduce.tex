\par{Roadmap: } This chapter introduces the KFAC approximation for weights (or combined weight and bias) of fully-connected layers (\ie a \texttt{torch.nn.Linear}).
Our discussion will heavily rely on regression settings with deep linear networks, \ie MLPs that consist of dense layers without any non-linearities.
We think they are a great setup to understand the core approximations of KFAC, and verify them numerically through rigorous tests.

We will start with the KFAC-expand approximation, which corresponds to the seminal KFAC approximation proposed by~\citet{martens2015optimizing}. It was introduced for standard MLP settings where the linear layers do not exhibit weight sharing (and therefore no reductions over weight-shared axes).
For more details on extensions such as KFAC-reduce see outlook or \citet{eschenhagen2023kroneckerfactored}.

\begin{setup}[Linear layer inside a neural network]\label{setup:linear_layer}
  Consider a linear layer with weight $\mW \in \sR^{D_{\text{out}}\times D_{\text{in}}}$ and bias $\vb \in \sR^{D_{\text{out}}}$ inside a neural network.
  Further, assume the network's prediction feeds into a criterion function and we use a data set of $N$ points to compute an empirical risk (with a reduction factor $R$) whose curvature matrix $\mC(\vec \mW)$ we are interested in computing.

  For each datum $n$, the layer processes a vector input $x_n \in \sR^{D {\text{in}}}$ into an output vector $\vz_{n} \in sR^{D_{\text{out}}}$ where
  $$ \vz_{n} = \mW \vx_{n} + \vb$$.
  Denote by $\mF_n \in \sR^{C \times}$ the network's prediction on datum $n$.
  For each vector $\blacktriangle_{n,c} \in \sR^{C \times S'}$ backpropagated by KFAC, define the errors arriving at the layer's output as $\vg_{n,c} = (\jac^{\vec}_{\vz_{n}} \mF_n)^{\top} \blacktriangle_{n,c} \in \sR^{D_{\text{out}}}$. These vectors do not depend on the flattening scheme because the contracted dimensions are summed out. We can group them into a matrix-values layer output gradient
  \begin{align*}
    \mG_{n,c}
    =
    \begin{pmatrix}
      \vg_{n,c} & \ldots & \vg_{n,c}
    \end{pmatrix}
    \in \sR^{D_{\text{out}}}\,.
  \end{align*}
  Most of the following discussion includes the case where the bias is included in the weight matrix by adding a constant input dimension to the input vectors:
  $$ \tilde{\mW} = \begin{bmatrix} W & b \end{bmatrix} \in \sR^{D_{\text{out}} \times (D_{\text{int}} + 1)}, \quad \tilde{\vx} = \begin{bmatrix} x \\ 1 \end{bmatrix} \in \sR^{D_{\text{in}} + 1} $$
  However, the corresponding code and the main discussion focus on the case where they are treated separately.
\end{setup}

\begin{caveat}
  In some cases the linear layer operates on a sequence of $S$ input vectors.
  For the following presentation of KFAC-expand, this could be resolved via flattening the sequence dimension into the batch dimension.
  A more relevant discussion is in the context of a specific treatment of weight-sharing, which will we will only briefly discuss in the outlook section.
  % The discussion of KFAC-expand naturally leads to an alternative setting where weight sharing dimensions are reduced during a forward pass; the so-called reduce setting.
  % For this setting, KFAC is not well motivated as it has no limit in which it becomes exact.
\end{caveat}

\subsection{KFAC-expand}

Let's consider the GGN for a linear layer with weights $\mW$, input vector $\vx$, and output vector $\vz = \mW \vx$ (suppressing layer indices for simplicity). Let's also assume $\cvec$-flattening, as usually done by the literature and make use of the output-parameter Jacobian of a linear layer (remember Example ??)
\begin{align*}
  \ggn_{\mW} \gL_{\sD}
  &=
    R
    \sum_n \sum_c
    (\jac^{\rvec}_{\mW}\vz_n)^{\top}
    (\jac^{\rvec}_{\vz_n}\vf_n)^{\top}
    \blacktriangle_{n,c}
    \blacktriangle_{n,c}^{\top}
    (\jac^{\rvec}_{\vz_n}\vf_n)
    (\jac^{\rvec}_{\mW}\vz_n)
  \\
  \shortintertext{
  (introduce $\vb_{n,c} \coloneq (\jac^{\rvec}_{\vz_n}\vf_n)^{\top} \blacktriangle_{n,c}$ for the layer output gradient)
  }
  &=
    R
    \sum_n \sum_c
    (\jac^{\rvec}_{\mW}\vz_n)^{\top}
    \vb_{n,c} \vb_{n,c}^{\top}
    (\jac^{\rvec}_{\mW}\vz_n)
    \shortintertext{Insert the linear layer's Jacobian}
  &=
    R
    \sum_n \sum_c
    (\vx_n \otimes \mI_{D_{\text{out}}})
    \vb_{n,c} \vb_{n,c}^{\top}
    (\vx_n^{\top} \otimes \mI_{D_{\text{out}}})
  \\
  &=
    R
    \sum_c
    \sum_n
    \vx_n \vx_n^{\top}
    \otimes
    \vb_{n,c} \vb_{n,c}^{\top}
\end{align*}

The expectation approximation in KFAC is the following:
\begin{align}
  \sum_n \va_n\va_n^{\top} \otimes \vb_n \vb_n^{\top}
  \approx
  \left( \sum_n \va_n \va_n^{\top} \right)
  \otimes
  \frac{
  \left( \sum_n \vb_n \vb_n^{\top} \right)
  }{N}
\end{align}
Instead of summing Kronecker products, we first sum the individual factors, then take their Kronecker product and divide by the number of summands. The scaling is by convention absorbed into the gradient-based Kronecker factor.

Our discussion will heavily rely on regression settings with deep linear networks, \ie MLPs that consist of dense layers without any non-linearities.
We think they are a great setup to understand the core approximations of KFAC, and verify them numerically through rigorous tests.

We will start with the KFAC-expand approximation, which corresponds to the seminal KFAC approximation proposed by~\citet{martens2015optimizing}. It was introduced for standard MLP settings where the linear layers do not exhibit weight sharing (and therefore no reductions over weight-shared axes).
We will then show how to extend KFAC-expand to the presence of weight sharing.

The discussion of KFAC-expand naturally leads to an alternative setting where weight sharing dimensions are reduced during a forward pass; the so-called reduce setting.
For this setting, KFAC is not well motivated as it has no limit in which it becomes exact.
As a solution, we will explain the alternative KFAC-reduce approximation, recently introduced by~\citet{eschenhagen2023kroneckerfactored}.

\subsection{KFAC-expand \& KFAC-reduce}


If we apply this approximation to the above, we obtain
\begin{align*}
  \kfac(\cvec \mW)
  &=
    \mA \otimes \mB
  \\
  \kfac(\rvec \mW)
  &=
    \mB \otimes \mA
    \shortintertext{where}
    \mA
  &=
    R
    \sum_n
    \vx_n \vx_n^{\top}
  \\
  \mB
  &=
    \frac{
    \left(
    \sum_n
    \sum_c
    \vb_{n,c} \vb_{n,c}^{\top}
    \right)
    }{N}
\end{align*}
Note that we absorbed the reduction factor of the loss into input-based factor.
Also note that the order of Kronecker factors changes depending on the flattening scheme.
The same equation holds if we treat weight and bias jointly, but we need to make the substitutions
\begin{align*}
  \mW \leftrightarrow \tilde{\mW}
  \quad
  \text{and}
  \quad
  \vx_n \leftrightarrow \begin{pmatrix} \vx_n \\ 1 \end{pmatrix}.
\end{align*}

\switchcolumn[1]
\codeblock{kfac/expand_Linear}
\switchcolumn[0]

\subsection{Tests}
A cental question for implementing KFAC is on how to actual test the code.

When does this approximation become exact?
Generally speaking, it becomes exact whenever one or both of the Kronecker factors in ??
does not depend on $n$.

\begin{definition}[KFAC-expand for a linear layer, \Cref{kfac/expand_Linear}]\label{def:kfac_expand_linear}
  Consider a linear layer inside a neural network from \Cref{setup:linear_layer}.
  The KFAC-expand approximation of a curvature matrix \wrt the layer's weights is given by
  \begin{subequations}\label{eq:kfac_expand_linear}
    \begin{align}
      \begin{split}
        &\kfac_{\text{exp}}(\vec \mW) \approx \mC(\vec \mW)
        \\
        &\quad=
          \begin{cases}
            \mA_{\text{exp}} \otimes \mB_{\text{exp}} & \vec = \cvec
            \\
            \mB_{\text{exp}} \otimes \mA_{\text{exp}} & \vec = \rvec
          \end{cases}\,,
      \end{split}
    \end{align}
    with the input-based factor $\mA_{\text{exp}} \in \sR^{D_{\text{in}} \times D_{\text{in}}}$
    \begin{align}
      \mA_{\text{exp}} &= R \sum_{n=1}^N\sum_{s=1}^S \vx_{n,s} \vx_{n,s}^{\top}
                         \intertext{and the output-gradient-based factor $\mB_{\text{exp}} \in \sR^{D_{\text{out}} \times D_{\text{out}}}$}
                         \mB_{\text{exp}} &= \frac{1}{NS}\sum_{n=1}^N \sum_{s=1}^S \sum_c \vg_{n,s,c} \vg_{n,s,c}^{\top}\,.
    \end{align}
  \end{subequations}
\end{definition}

\switchcolumn[1]
\codeblock{kfac/reduce_Linear}
\switchcolumn[0]

\begin{definition}[KFAC-reduce approximation for a linear layer with weight sharing, \Cref{kfac/reduce_Linear}]\label{def:kfac_reduce_linear}
  Consider a linear layer inside a neural network from \Cref{setup:linear_layer}.
  The KFAC-reduce approximation for a curvature matrix \wrt the layer's weights is given by
  \begin{subequations}\label{eq:kfac_reduce_linear}
    \begin{align}
      \begin{split}
        &\kfac_{\text{red}}(\vec \mW) \approx \mC(\vec \mW)
        \\
        &\quad=
          \begin{cases}
            \mA_{\text{red}} \otimes \mB_{\text{red}} & \vec = \cvec
            \\
            \mB_{\text{red}} \otimes \mA_{\text{red}} & \vec = \rvec
          \end{cases}\,,
      \end{split}
    \end{align}
    with the input-based factor $\mA_{\text{red}} \in sR^{D_{\text{in}}\times D_{\text{in}}}$
    \begin{align}
      \begin{split}
        \mA_{\text{red}}
        &=
          R \sum_{n=1}^N
          \left( \sum_{s=1}^S \vx_{n,s} \right)
          \left( \sum_{s'=1}^S \vx_{n,s'} \right)^{\top}
        \\
        &=
          R \sum_{n=1}^N
          \left( \mX_n \vone_S \right)
          \left( \mX_n \vone_S \right)^{\top}
      \end{split}
      \intertext{and the output-gradient-based factor $\mB_{\text{red}} \in \sR^{D_{\text{out}}\times D_{\text{out}}}$}
      \begin{split}
        \mB_{\text{red}}
        &=
          \frac{1}{N S^2} \sum_{n=1}^N \sum_c
        \\
        &\qquad
          \left( \sum_{s=1}^S \vg_{n,s,c} \right)
          \left( \sum_{s'=1}^S \vg_{n,s',c} \right)^{\top}
        \\
        &= \frac{1}{N S^2} \sum_{n=1}^N \sum_c
          \left(\mG_{n,c} \vone_S \right)
          \left(\mG_{n,c} \vone_S \right)^{\top}
      \end{split}
    \end{align}
  \end{subequations}
\end{definition}

Comments on \Cref{def:kfac_reduce_linear,def:kfac_expand_linear}:
\begin{itemize}

\item The main difference between KFAC-expand and KFAC-reduce is how they treat the shared dimension.
  KFAC-expands treats the shared dimension independently and like a batch dimension.
  KFAC-reduce first accumulates the layer inputs and output-gradients over their shared dimension.
  In the absence of weight sharing, $S=1$, both approximations are equivalent.

\item Depending on which vectors we choose for backpropagation, we obtain the type-II, MC, empirical, and input-only KFAC-expand/reduce flavours $\kfac_{\text{red, exp}}^{\text{II}}$, $\kfac_{\text{red, exp}}^{\text{MC}}$, $\kfac_{\text{red, exp}}^{\text{E}}$ and $\kfac_{\text{red,exp}}^{\text{IO}}$.

\item If we treat weight and bias jointly via $\tilde{\mW} = \begin{pmatrix} \mW & \vb \end{pmatrix} \in \sR^{D_{\text{out}}\times (D_{\text{in}}+1)} $, we need to make the following substitutions in \Cref{eq:kfac_expand_linear} and $\mA_{\text{exp, red}} \in \sR^{(D_{\text{in}}+1) \times (D_{\text{in}}+1)}$:
  \begin{align*}
    \mW \leftrightarrow \tilde{\mW}
    \quad
    \text{and}
    \quad
    \vx_{n,s} \leftrightarrow \begin{pmatrix} \vx_{n,s} \\ 1 \end{pmatrix}\,.
  \end{align*}

\item By convention, the reduction factor $R$ is absorbed into the input-based Kronecker factor.
  This, as well as the summation over $c$, is done by the scaffold in \Cref{kfac/scaffold} and not by the layer-specific implementations in \Cref{kfac/expand_Linear,kfac/reduce_Linear} because the layer has no knowledge of $R$.
  This guarantees that we can safely set the output-gradient-based factor to the identity matrix for the input-only variation and still be able to recover the correct curvature matrix in special cases, even when changing the loss function's reduction.
\end{itemize}

\subsection{Algebraic Structure of a Curvature Matrix}
To try to get an intuition for the approximations introduced in the previous sections, let's take a closer look at the equation for a curvature matrix \wrt a linear layer inside a neural network from \Cref{setup:linear_layer},
\begin{align*}
  &\mC( \vec \mW)
  \\
  &=
    R\sum_{n=1}^N \sum_c
    (\jac_{\mW}^{\vec}\mZ_n)^{\top}
    (\jac_{\mZ_n}^{\vec}\mF_n)^{\top}
    (\vec \blacktriangle_{n,c})
  \\
  &\qquad\quad
    (\vec \blacktriangle_{n,c})^{\top}
    (\jac_{\mZ_n}^{\vec}\mF_n)
    (\jac_{\mW}^{\vec}\mZ_n)
  \\
  &=
    R\sum_{n=1}^N \sum_c \sum_{s=1}^S
    (\jac_{\mW}^{\vec}\vz_{n,s})^{\top}
    (\jac_{\vz_{n,s}}^{\vec}\mF_n)^{\top}
    (\vec \blacktriangle_{n,c})
  \\
  &\qquad\quad
    (\vec \blacktriangle_{n,c})^{\top}
    (\jac_{\vz_{n,s}}^{\vec}\mF_n)
    (\jac_{\mW}^{\vec}\vz_{n,s})
    \intertext{where we simply extracted the summation over the shared dimension by taking Jacobians \wrt the vectors in a sequence.
    Let's introduce the shortcut $\vg_{n,s,c} = (\jac_{\vz_{n,s}}^{\vec}\mF_n)^{\top} (\vec \blacktriangle_{n,c})$ for the gradient arriving at the layer's output for each vector of the sequence.
    It does not depend on $\vec$.}
  &=
    \sum_{n=1}^N \sum_c \sum_{s=1}^S
    (\jac_{\mW}^{\vec}\vz_{n,s})^{\top}
    \vg_{n,s,c}
    \vg_{n,s,c}^{\top}
    (\jac_{\mW}^{\vec}\vz_{n,s})
\end{align*}
Next, let's set $\vec = \cvec$ and insert the linear layer's output-weight Jacobian to get
\begin{align*}
  &\mC(\cvec \mW)
  \\
  &=
    R\sum_{n=1}^N \sum_c \sum_{s=1}^S
    \left( \vx_{n,s} \otimes \mI \right)
    \vg_{n,s,c}
    \vg_{n,s,c}^{\top}
    \left( \vx_{n,s}^{\top} \otimes \mI \right)
  \\
  &=
    R\sum_{n=1}^N \sum_c \sum_{s=1}^S
    \vx_{n,s} \vx_{n,s}^{\top} \otimes
    \vg_{n,s,c} \vg_{n,s,c}^{\top}
  \\
  &=
    R\sum_{n=1}^N \sum_c
    \left( \mX_{n} \otimes \mI \right)
    \cvec\mG_{n,c}
    (\cvec \mG_{n,c})^{\top}
    \left( \mX_{n}^{\top} \otimes \mI \right)
\end{align*}

The curvature matrix is a sum of Kronecker products.
We want to approximate this with a single Kronecker product.
To do that, we need to agree on a convention to carry out the sum over $n$ before taking the Kronecker product. We will use:
\begin{definition}[KFAC's expectation approximation]
  Consider two sets of $N$ vectors $\{\vx_{n}\}_{n=1}^N$ and $\{\vg_{n}\}_{n=1}^N$ where each $\vx_n$ is input-based and each $\vg_n$ is output-gradient-based.
  KFAC approximates the summed Kronecker products of the self-outer products $\vx_n \vx_n^{\top} \otimes \vg_n \vg_n^{\top}$ as a single Kronecker products using the following expectation approximation:
  \begin{subequations}\label{eq:expectation_approximation}
    \begin{align}
      \begin{split}
        &\sum_{n=1}^{\textcolor{VectorOrange}{N}} \vx_n\vx_n^{\top} \otimes \vg_n \vg_n^{\top}
        \\
        &\qquad\approx
          \left( \sum_{n=1}^N \vx_n \vx_n^{\top} \right)
          \otimes
          \frac{
          \left( \textcolor{VectorOrange}{\sum_n} \vg_n \vg_n^{\top} \right)
          }{\textcolor{VectorOrange}{N}}
      \end{split}
      \intertext{In words, we sum the outer products on the left and right of the Kronecker product independently and divide by the number of outer products in the term containing the gradient vectors.
      In expectation notation (expectation meaning averaging over all values of $n$, \ie $\E_n[\bullet_n] = \nicefrac{1}{N} \sum_{n=1}^N \bullet_n$), we can write this approximation more conveniently as}
      \begin{split}
        &\E_n \left[ \vx_n \vx_n^{\top} \otimes \vg_n \vg_n^{\top} \right]
        \\
        &\qquad\approx
          \E_n \left[ \vx_n \vx_n^{\top} \right]
          \otimes
          \E_n \left[ \vg_n \vg_n^{\top} \right]\,,
      \end{split}
    \end{align}
  \end{subequations}
  \ie `pull' the expectation into the Kronecker factors.
  If $\vx$ and $\vg$ were independent, this approximation would be exact.
\end{definition}

\begin{example}[Justification for KFAC's expectation approximation]
  KFAC's expectation approximation can be derived from an optimality condition under special assumptions introduced to preserve a Kronecker structure.

  Assume we are given the expression
  \begin{align*}
    \mC
    &=
      R \sum_{n=1}^N
      (\vx_n \otimes \mI_{D_{\text{out}}})
      \vg_n \vg_n^{\top}
      (\vx_n^{\top} \otimes \mI_{D_{\text{out}}})
    \\
    &=
      R \sum_{n=1}^N
      \vx_n \vx_n^{\top} \otimes \vg_n \vg_n^{\top}
  \end{align*}
  with $\vx_n \in \sR^{D_{\text{in}}}$ and $\vg_n \in \sR^{D_{\text{out}}}$ and our goal is to approximate $\mC$ through a single Kronecker product.
  Let's first make this expression more compact by absorbing the summation over data points in to matrix multiplication,
  \begin{align*}
    \mC =
    R (\mX \otimes \mI_{D_{\text{out}}})
    \cvec(\mG) \cvec(\mG)^{\top}
    (\mX^{\top} \otimes \mI_{D_{\text{out}}})
  \end{align*}
  where we simply stack the vectors $\{ \vx_n \}$ and $\{\vg_n \}$ into matrices
  \begin{align*}
    \mX
    &=
      \begin{pmatrix}
        \vx_1 & \vx_2 & \ldots & \vx_N
      \end{pmatrix}
      \in \sR^{D_{\text{in}} \times N}\,,
    \\
    \mG
    &=
      \begin{pmatrix}
        \vg_1 & \vg_2 & \ldots & \vg_N
      \end{pmatrix}
      \in \sR^{D_{\text{out}} \times N}\,.
  \end{align*}
  Looking at the expression for $\mC$, we realize that an easy way to achieve a single Kronecker product is if we could somehow approximate $\cvec(\mG) \cvec(\mG)^{\top} \approx \mI_{D_{\text{in}}} \otimes \mB$ with $\mB \in \sR^{D_{\text{out}}\times D_{\text{out}}}$, because then we could write
  \begin{align*}
    \mC
    &\approx
      R (\mX \otimes \mI_{D_{\text{out}}})
      (\mI_{D_{\text{in}}} \otimes \mB)
      (\mX^{\top} \otimes \mI_{D_{\text{out}}})
    \\
    &=
      R \mX \mX^{\top} \otimes \mB
      =
      \mA \otimes \mB\,,
  \end{align*}
  which is what we would like to achieve.

  How do we find $\mB$? By minimizing the squared Frobenius norm residual
  \begin{align*}
    &\argmin_{\mB}
      \left\lVert
      \cvec(\mG) \cvec(\mG)^{\top} - \mI_{D_{\text{in}}} \otimes \mB
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \left\lVert
      \scalebox{0.75}{%
      $
      \begin{pmatrix}
        \vg_1 \vg_1^{\top} -\mB & \vg_1 \vg_2^{\top} & \dots & \vg_1 \vg_N^{\top} \\
        \vg_2 \vg_1^{\top} & \vg_2 \vg_2^{\top} - \mB & \ddots & \vg_2 \vg_N^{\top} \\
        \vdots & \ddots & \ddots & \vdots \\
        \vg_N \vg_1^{\top} & \vg_N \vg_2^{\top} & \dots & \vg_N \vg_N^{\top} - \mB \\
      \end{pmatrix}
      $
      }
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \left\lVert
      \scalebox{0.75}{%
      $
      \begin{pmatrix}
        \vg_1 \vg_1^{\top} - \mB & \vzero & \dots & \vzero \\
        \vzero & \vg_2 \vg_2^{\top} - \mB & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \vzero \\
        \vzero & \hdots & \vzero & \vg_N \vg_N^{\top} - \mB \\
      \end{pmatrix}
      $
      }
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \sum_{n=1}^N
      \left\lVert
      \vg_n \vg_n^{\top} - \mB
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \sum_{n=1}^N
      \sum_{i,j = 1}^{D_{\text{out}}}
      \left(
      [\vg_n]_i [\vg_n]_j - [\mB]_{i,j}
      \right)^2 \,.
  \end{align*}
  Taking the derivative \wrt $\mB$ and setting it to zero yields
  \begin{align*}
    &\frac{\partial \left(
      \sum_{n=1}^N
      \sum_{i,j = 1}^{D_{\text{out}}}
      \left(
      [\vg_n]_i [\vg_n]_j - [\mB]_{i,j}
      \right)^2
      \right)
      }{
      \partial [\mB]_{k,l}
      }
    \\
    &=
      2 \sum_{n=1}^N
      \left(
      [\mB]_{k,l} -  [\vg_n]_k [\vg_n]_l
      \right)
    \\
    &=
      2 \left( N[\mB]_{k,l} - \sum_{n=1}^N [\vg_n]_k [\vg_n]_l \right)
      \stackrel{!}{=} 0 \qquad\forall k,l
    \\
    &\implies
      \mB = \frac{1}{N} \sum_{n=1}^N \vg_n \vg_n^{\top}
      = \frac{1}{N} \mG \mG^{\top}\,.
  \end{align*}
  Hence, if we `model' the gradient covariance as block-diagonal matrix with identical blocks $\mB$, the `best' choice is the average of gradient outer products, just like in KFAC,
  \begin{align*}
    \mC
    \approx
    \left(R \sum_n \vx_n \vx_n^\top \right)
    \otimes
    \left(\frac{1}{N} \sum_{n'} \vg_{n'} \vg_{n'}^\top \right)\,.
  \end{align*}
\end{example}

When does the approximation in \Cref{eq:expectation_approximation} become exact?
There are two cases, one of which is relatively obvious.
Generally speaking, the approximation becomes exact whenever one of $\{\va_n\}$or $\{\vg_n\}$ are data-independent, \ie do not depend on $n$.


\switchcolumn[1]*
\codeblock{kfac_tests/expand_mlp_batch_size_1}
\switchcolumn[0]

\subsubsection{Test Case: Data Set Size 1}
One obvious case is whenever the summation over $n$ disappears, \ie our data set contains only a single data point (or $N$ identical data).

\begin{test}[KFAC-expand for linear layers in an MLP (no weight sharing), one data point, \Cref{kfac_tests/expand_mlp_batch_size_1}]\label{test:kfac_expand_linear_no_weight_sharing_batch_size_1}
  Consider a multi-layer perceptron
  \begin{align*}
    f = \phi^{(L)} \circ f^{(L)} \circ \ldots \circ \phi^{(1)} \circ f^{(1)}
  \end{align*}
  that processes a vector-valued input $\vx$ through a sequence of layers, each of which consists of a dense layer $f^{(l)}$ and a pointwise activation $\phi^{(l)}$.
  Let $\mW^{(l)}$, $\vb^{(l)}$, and $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$ denote the weight, bias, and combined parameters of a dense layer $f^{(l)}$.
  Further, assume the data set consists only of a single point, $\sD = \{ (\vx, \vy) \}$, and a criterion function $c = - \log r$ which can be interpreted as negative log likelihood.
  Then, KFAC-expand becomes exact in the following limits for $l = 1, \dots, L$, some flattening scheme $\vec$, and also when swapping the combined weights with only the weight matrix:
  \begin{itemize}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec\tilde{\mW}^{(l)}) = \ggn^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec\tilde{\mW}^{(l)})
      \\
      = \ggn^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \item KFAC-empirical equals the EF
    \begin{align*}
      \kfac^{\text{E}}_{\text{exp}}(\vec\tilde{\mW}^{(l)}) = \ef^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \end{itemize}
\end{test}
This test is useful for checking functionality related to the criterion function, \ie the backpropagated vectors $\blacktriangle_{n,c}$
It can further be generalized: We can replace the activation functions with arbitrary layers (\eg\,reshape, convolution, pooling, normalization, \dots) as long as the inputs processed by the dense layers remain vector-shape.
These changes do not alter the structure of the GGN/Fisher/EF for the linear layers.

\switchcolumn[1]*
\codeblock{kfac_tests/input_only_last_layer_regression}
\switchcolumn[0]

\subsubsection{Test Case: Last layer FOOF}

\begin{test}[Last-layer FOOF for a linear layer without weight sharing~\cite{petersen2023isaac}, \Cref{kfac_tests/input_only_last_layer_regression}]
  Consider the MLP from \Cref{test:kfac_expand_linear_no_weight_sharing_batch_size_1}, but set $\phi^{(L)} = \mathrm{id}$ to identity, $c$ to square loss, and allow for a data set of arbitrary size $N$.
  Then we have the following exact relations between curvature matrices and KFAC flavours for the last layer:
  \begin{itemize}
  \item KFAC-expand-input-only equals the GGN
    \begin{align*}
      \kfac^{\text{IO}}_{\text{exp}}(\vec\tilde{\mW}^{(L)})
      &=
        \ggn^{\vec}_{\tilde{\mW}^{(L)}}\gL_{\sD}
        % \\
        % &=
        % \mA \otimes \mI
    \end{align*}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      TODO
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      TODO
    \end{align*}
  \item Importantly, KFAC-empirical does \emph{not} equal the empirical Fisher.
    \begin{align*}
      TODO
    \end{align*}
    This is because its backpropagated vectors $\blacktriangle_{n,c}$ depend on the datum $n$, while the other two flavours use vectors that do not depend on $n$.
  \end{itemize}
\end{test}

\switchcolumn[1]*
\codeblock{kfac_tests/expand_deep_linear_regression}
\switchcolumn[0]

\subsubsection{Test Case: Regression with Deep Linear Networks}

The more subtle and interesting testing scenario where the KFAC approximation becomes exact is for deep linear networks and square loss.
In this case $\vg_{n,c}$ does not depend on $n$.
To see this, let's look at the definition of $\vg_{n,c}$ which depends on the backpropagated vector $\blacktriangle_{n,c}$ and the Jacobian of the prediction \wrt the layer's output $\jac_{\vz_n} \vf_n$.
For square loss, $\blacktriangle_{n,c}$ often does not depend on $n$: For type-2, it is simply a column of the identity matrix, and for MC, it is simply a normally distributed random number.
If we further restrict all layers of our neural network to be linear layers all prediction-intermediate Jacobians are constant and therefore independent of $n$.
This implies the following limit in which KFAC becomes exact (first proposed in \cite{bernacchia2018exact}).

\begin{test}[KFAC-expand for regression with deep linear networks without weight sharing, \Cref{kfac_tests/expand_deep_linear_regression}]
  Consider a deep linear network consisting of $L$ dense layers
  \begin{align*}
    f = f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)}
  \end{align*}
  with weights $\mW^{(l)}$, bias $\vb^{(l)}$, and combined parameters $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$.
  Assume the network processes a vector-valued input per datum and consider regression with square loss on an arbitrary data set $\sD = \{ (\vx_n, \vy_n) \mid n = 1, \dots, N \}$.
  Then, we have the following equalities for $l = 1, \dots, L$ and a flattening scheme $\vec$ of our choice:
  \begin{itemize}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      =
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      \\=
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}

  \end{itemize}
  Note that KFAC-expand-empirical does \emph{not} equal the empirical Fisher,
  \begin{align*}
    \kfac_{\text{exp}}^{\text{E}}(\vec \tilde{\mW}^{(l)})
    \neq
    \ef^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}\,,
  \end{align*}
  because its backpropagated vector $\blacktriangle_{n,c}$ depends on $\vf_n$ and therefore on $n$.
\end{test}
We can generalize this test further and allow for arbitrary operations that operate on an input in a linear fashion, \eg convolutions, reshapes, padding, average pooling, as long as the input to a dense layer remains vector-valued.
We cannot pick non-linear layers like ReLU, sigmoid, max-pooling, or normalization, as their output-input Jacobian is input-dependent.

\subsection{KFAC-expand with Weight Sharing}

What happens if the input to our neural network is not a single vector, but a collection of vectors (\ie a matrix)? Assume a linear layer is processing a sequence of vectors $\mX_n =
\begin{pmatrix} \vx_{n,1} & \dots & \vx_{n,S} \end{pmatrix}$ into a sequence of output vectors $\mZ_n =
\begin{pmatrix} \vz_{n,1} & \dots & \vz_{n,S} \end{pmatrix}$ for a datum $n$ where each $\vz_{n,s} = \mW \vx_{n,s}$.
We say that these vectors \emph{share} the same weight.
It will turn out that we can treat this shared axis just like a batch axis for the KFAC-expand approximation:

\begin{definition}[KFAC-expand for a linear layer with matrix-valued input, \Cref{kfac/expand_Linear}]\label{def:kfac_expand_linear_weight_sharing}
  Consider a linear layer with weight $\mW$ and bias $\vb$ inside a neural network that processes a sequence of $S$ vectors arranged into a matrix-valued input $\mA_n = \begin{pmatrix} \va_{n,1} & \dots & \va_{n,S} \end{pmatrix}$ for each datum $n$ into a vector-valued output $\mZ_n = \mW \mA_n + \vb \vone_S^{\top} = \begin{pmatrix} \vz_{n,1} & \dots & \vz_{n,S} \end{pmatrix}$. The KFAC-expand approximation of a curvature matrix $\mC(\cvec \mW)$ in column-flattening convention is given by
  \begin{subequations}
    \begin{align}
      \begin{split}
        &\kfac_{\text{exp}}(\cvec \mW)
        \\
        &\quad=
          \mA \otimes \mB
          \approx \mC(\cvec \mW)\,,
      \end{split}
      \intertext{where}
      \begin{split}
        &\mA = R \sum_{n,s} \va_{n,s} \va_{n,s}^{\top}\,,
        \\
        &\mB = \frac{\left(\sum_{n,s} \sum_c \vg_{n,s,c} \vg_{n,s,c}^{\top} \right)}{N S}\,.
      \end{split}
    \end{align}
  \end{subequations}
  The reduction factor from the loss function is absorbed into the input-based Kronecker factor (this is done by the scaffold in \Cref{kfac/scaffold}).
  The shared dimension is treated exactly like a batch dimension.

  If we treat weight and bias jointly, we need to make the substitutions
  \begin{align*}
    \mW \leftrightarrow \tilde{\mW}
    \quad
    \text{and}
    \quad
    \va_n \leftrightarrow \begin{pmatrix} \va_n \\ 1 \end{pmatrix}\,.
  \end{align*}
\end{definition}
There are different ways to argue why simply treating the shared dimension as a batch dimension is reasonable. Let's write down the exact curvature matrix first,
\begin{align*}
  &\mC(\cvec \mW)
  \\
  &\quad=
    R
    \sum_n \sum_c
    (\jac^{\cvec}_{\mW}\mZ_n)^{\top}
  \\
  &\quad
    \cvec\mG_{n,c} (\cvec\mG_{n,c})^{\top}
    (\jac^{\cvec}_{\mW}\mZ_n)
    \shortintertext{After inserting the linear layer's output-weight Jacobian from \Cref{ex:weight_jacobians_linear_layer}, we get}
  &\quad=
    R
    \sum_n \sum_c (\mA_n \otimes \mI_{\vz_n})
  \\
  &\quad\quad
    \cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top}
    (\mA_n^{\top} \otimes \mI_{\vz_n})\,.
\end{align*}
This does not simplify into a single Kronecker product!
What additional approximation could we make to simplify this expression?
Note that the matrix $\vec\mG_{n,c} (\vec\mG_{n,c})^{\top}$ is $S D_{\text{out}} \times S D_{\text{out}}$. If we approximate it via a Kronecker product $\cvec\mG_{n,c} (\cvec\mG_{n,c})^{\top} \approx \mK \otimes \mI_{D_\text{out}}$, we could simplify the structure above into a sum of a Kronecker product. It is easy to show that
\begin{align*}
  \argmin_{\mK}
  \left\lVert
  \cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top} - \mK \otimes \mI_{D_{\text{out}}}
  \right\rVert_2
  \\
  =
  \frac{1}{S} \mG_{n,c} \mG_{n,c}^{\top}
\end{align*}
\ie we average the diagonal blocks of size $S \times S$.
Using this Kronecker approximation of the backpropagated vector's self-outer product allows us to simplify the curvature matrix into
\begin{align*}
  \mC(\cvec\mW)
  \approx
  R \sum_n \sum_c
  \mA_n \mA_n^{\top} \otimes \frac{1}{S} \mG_{n,c} \mG_{n,c}^{\top}
  \\
  = R \sum_n \sum_s \va_{n,s} \va_{n,s}^{\top} \otimes \sum_{s'} \sum_c \frac{1}{S} \vg_{n,s',c} \vg_{n,s',c}^{\top}
  \intertext{If we next use the expectation approximation we agreed upon in \Cref{eq:expectation_approximation} to pull the sum over $n$ inside each of the Kronecker factors, we obtain}
  \approx
  \left(R \sum_n \sum_s \va_{n,s} \va_{n,s}^{\top}\right)
  \\
  \otimes \left( \frac{\textcolor{VectorOrange}{\sum_n} \textcolor{VectorTeal}{\sum_s}\sum_c \vg_{n,s,c} \vg_{n,s,c}^{\top}}{\textcolor{VectorOrange}{N} \textcolor{VectorTeal}{S}} \right)
\end{align*}
From this equation we can see that the shared dimension (highlighted in teal) is treated exactly like a batch dimension (highlighted in orange) in KFAC's expectation approximation.

Some observations on the matrix $\cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top}$ while writing this: If the shared dimension is processed independently, then this matrix is block diagonal.
But each $S \times S$ block is different.
If the shared dimension is processed independently, and all layers are linear, then the matrix is block-diagonal and all $S \times S$ blocks on the diagonal are identical.
This is the next test case for KFAC.

\subsubsection{Test Case: Regression with Deep Linear Networks and Weight Sharing}\label{sec:expand_sharing}

If each shared dimension is processed independently, we can treat it just like a batch dimension.
This is the case if we use an MLP that consists of dense layers and element-wise activation functions.
The test in \Cref{kfac_tests/expand_deep_linear_regression} includes such a scenario where $S > 1$.
Note however that in practise we want to combine the features in a sequence (\eg\,as done in the attention layer).
But for a simple MLP, the shared axis is just treated like a batch axis, which motivates treating it as such in the KFAC-expand approximation.

\begin{test}[KFAC-expand for regression with deep linear networks with weight sharing, \Cref{kfac_tests/expand_deep_linear_regression}]
  Consider a deep linear network consisting of $L$ dense layers
  \begin{align*}
    f = f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)}
  \end{align*}
  with weights $\mW^{(l)}$, bias $\vb^{(l)}$, and combined parameters $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$.
  Assume the network processes a matrix-valued input per datum and consider regression with square loss on an arbitrary data set $\sD = \{ (\mX_n, \mY_n) \mid n = 1, \dots, N \}$.
  Then, we have the following equalities for $l = 1, \dots, L$ and a flattening scheme $\vec$ of our choice:
  \begin{itemize}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      =
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      \\=
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}

  \end{itemize}
  Note that KFAC-expand-empirical does \emph{not} equal the empirical Fisher,
  \begin{align*}
    \kfac_{\text{exp}}^{\text{E}}(\vec \tilde{\mW}^{(l)})
    \neq
    \ef^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}\,,
  \end{align*}
  because its backpropagated vector $\blacktriangle_{n,c}$ depends on $\vf_n$ and therefore on $n$.
\end{test}

\subsection{KFAC-reduce}
What happens in the presence of weight sharing if the shared axis is reduced at some point during the forward pass (either by summing or averaging over it)?

Consider the following setting: Consider a deep linear network
\begin{align*}
  f = \rho \circ f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)}
\end{align*}
with linear layers $f^{(l)}$ with weights $\mW^{(l)}$ and bias $\vb^{(l)}$ that process a matrix-valued input which is finally reduced over the shared dimension via summation or averaging ($\rho$):
\begin{align*}
  \rho: \mA \in^{D \times S}
  \mapsto
  \rho(\mA)
  \in \sR^D
  =
  \begin{cases}
    \mA \vone_S
    \\
    \frac{1}{S} \mA \vone_S
  \end{cases}
\end{align*}
Note that this is different from the expand setting where the shared dimension persists until the end of the forward pass and is only reduced to a scalar inside the criterion function $c = - \log r$.

The Jacobian of the reduction function has a Kronecker structure,
$\jac_{\mA}^{\rvec}\rho(\mA) = \vone_S^{\top} \otimes \mI_D$ and $\jac_{\mA}^{\cvec}\rho(\mA) = \mI_D \otimes \vone_S^{\top}$.
TODO Describe how this Kronecker structure affects the KFAC approximation


\paragraph{Why are deep linear networks great?}
Deep linear networks are great because each layer's output-input Jacobian, and therefore the Jacobian of the output \wrt any intermediate feature, is constant and therefore data-independent.
For a curvature matrix of structure ??, this means that the data dependency only enters in two places: (i) in the vectors $\blacktriangle_{n,c}$ that are generated for backpropagation, and (ii) in the Jacobian of a layer output \wrt its weight.
Consider a deep linear network $f = f^{(L)} \circ \ldots \circ f^{(1)}$ with $L$ layers with weights $\mW^{(l)}$ and bias $\vb^{(l)}$. For an arbitrary intermediate $\vx^{(l)}$ (the output of layer $l$), the Jacobian of the output \wrt the weight of layer $l$ is given by
\begin{align*}
  \jac_{\vx^{(l)}} f(\vx)
  =
  \mW^{(L)} \mW^{(L-1)} \ldots \mW^{(l+1)}
\end{align*}
(note that this does not depend on the data).
In the presence of weight sharing, we have
\begin{align*}
  \jac^{\cvec}_{\mX^{(l)}} f(\mX)
  &=
    \left(
    \mW^{(L)} \mW^{(L-1)} \ldots \mW^{(l+1)}
    \right)
    \otimes \mI_S
    \shortintertext{and}
    \jac^{\rvec}_{\mX^{(l)}} f(\mX)
  &=
    \mI_S
    \otimes
    \left(
    \mW^{(L)} \mW^{(L-1)} \ldots \mW^{(l+1)}
    \right)\,.
\end{align*}
In the presence of weight sharing and reduction, we have
\begin{align*}
  \jac^{\cvec}_{\mX^{(l)}} f(\mX)
  &=
    \left(
    \mW^{(L)} \mW^{(L-1)} \ldots \mW^{(l+1)}
    \right)
    \otimes \vone_S^{\top}
    \shortintertext{and}
    \jac^{\rvec}_{\mX^{(l)}} f(\mX)
  &=
    \vone_S^{\top}
    \otimes
    \left(
    \mW^{(L)} \mW^{(L-1)} \ldots \mW^{(l+1)}
    \right)\,.
\end{align*}

\switchcolumn[1]*
\codeblock{kfac_tests/deep_linear_net}
\switchcolumn[0]

\begin{setup}[Deep linear network w/o weight sharing and reduction, \Cref{kfac_tests/deep_linear_net}]
  Consider a deep linear network that consists of $L$ layers with weights $\mW^{(l)}$ and biases $\vb^{(l)}$
  \begin{align*}
    f = \rho \circ f^{(L)} \circ \ldots \circ f^{(1)}\,,
  \end{align*}
  followed by a final reduction layer $\rho$. For each datum $n$, the network processes a sequence of $S$ vectors $\vx_{n,1}, \dots, \vx_{n,S}$ arranged into a matrix-valued input $\mX^{(0)}_n = \begin{pmatrix} \vx_{n,1} & \ldots & \vx_{n,S} \end{pmatrix} \in \sR^{D_{\text{in}} \times S}$.
  For $S=1$, each layer only processes a single vector and we say that there is \emph{no weight sharing}, because each layer weight is only applied once to a vector.
  For $S>1$, each layer processes multiple vectors and we say that there is \emph{weight sharing}, because the same weight is applied to multiple vectors.
  The output $\mX^{(L)}_n$of the last linear layer can be reduced over the shared dimension via summation or averaging, or left unchanged by the operation $\rho$:
  \begin{align*}
    \rho(\mX_n^{(L)})
    &=
      \begin{cases}
        \mX_n^{(L)} & \text{no reduction}
        \\
        \mX_n^{(L)} \vone_S & \text{sum reduction}
        \\
        \frac{1}{S} \mX_n^{(L)} \vone_S & \text{mean reduction}
      \end{cases}
    \\
    &\in
      \begin{cases}
        \sR^{C \times S} & \text{no reduction}
        \\
        \sR^C & \text{sum reduction}
        \\
        \sR^C & \text{mean reduction}
      \end{cases}
  \end{align*}
  In the absence of weight sharing ($S = 1$), the reduction does not do anything ($\rho = \mathrm{id}$).
\end{setup}

\switchcolumn[1]*
\codeblock{kfac_tests/reduce_deep_linear_regression}
\switchcolumn[0]
\subsubsection{Test Case: Regression with Deep Linear Networks and Weight Sharing and Final Reduction}

\subsubsection{Test Case: Regression with Deep Linear Networks and Weight Sharing and Intermediate Reduction}

\subsection{KFAC's expectation approximation}

\subsection{KFAC's `Kroneckerization' factorization}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
