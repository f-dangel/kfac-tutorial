This is an attempt to bundle scattered knowledge about KFAC into a single document, explain all the technicalities and pitfalls, and present tests to ensure bug-free implementations.

Should answer the following questions:
\begin{itemize}
\item Why do we need this tutorial?
\item Why is this not a Jupyter notebook?
\item What do we gain by explaining KFAC bottom-up?
\item What ML framework do we use and why?
\item What are we \emph{not} doing (e.g.\,building an optimizer)?
\end{itemize}

We use PyTorch~\cite{paszke2019pytorch} and implement everything using \texttt{torch.nn} rather than a functional formulation as we feel that many deep learning practitioners are more familiar with this style.
There could be a JAX or functorch version, too.

\begin{itemize}
\item KFAC approximates the Fisher, which is an outer product of gradients. The gradients involve the output-parameter Jacobian of a weight, which has Kronecker structure,
  \begin{align}
    \jac_{\mW}(\mW \vx) = \vx^{\top} \otimes \mI\,.
  \end{align}
  This implies that the Fisher contains terms of the following form, where $\bullet$ is a placeholder for some matrix,
  \begin{align}
    \left(
    \vx^{\top}\otimes \mI
    \right)^{\top}
    \bullet
    \left(
    \vx^{\top}\otimes \mI
    \right)\,.
  \end{align}

\item The goal is to build up to an abstract general formulation of KFAC.
  Given a compute graph which uses the operations $\vx, \mW \mapsto \mW x$, potentially in multiple places, define a Kronecker approximation of the Fisher.
  Also to show the different degrees of freedom: treating weights \& biases jointly/separately, using reduce versus expand approximation, and treating weight tying, i.e.
  multi-usage of $\mW$.

\end{itemize}
TODO Should mention somewhere that many of the provided examples are already discussed in Felix's PhD thesis~\cite{dangel2023backpropagation}.

\paragraph{KFAC flavours} There are various degrees of freedom that induce different KFAC flavours:
\begin{align*}
  \begin{array}{c}
    \begin{Bmatrix}
      \text{PyTorch}
      \\
      \text{JAX}
    \end{Bmatrix}
    \\
    \text{(software framework)}
  \end{array}
  \times
  \begin{array}{c}
    \begin{Bmatrix}
      \rvec
      \\
      \cvec
    \end{Bmatrix}
    \\
    \text{(flattening convention)}
  \end{array}
  \times
  \begin{array}{c}
    \begin{Bmatrix}
      \text{MC (type-I Fisher)}
      \\
      \text{type-II Fisher}
      \\
      \text{empirical Fisher}
    \end{Bmatrix}
    \\
    \text{(target curvature)}
  \end{array}
  \times
  \begin{array}{c}
    \begin{Bmatrix}
      \text{expand}
      \\
      \text{reduce}
    \end{Bmatrix}
    \\
    \text{(how weight sharing is handled)}
  \end{array}
  \times
  \begin{array}{c}
    \begin{Bmatrix}
      \text{fully-connected}
      \\
      \text{convolutions}
    \end{Bmatrix}
    \\
    \text{(layers)}
  \end{array}
\end{align*}
The original work from~\citet{martens2015optimizing} uses
\begin{align*}
  \text{cvec} \times \text{MC (type-I Fisher)} \times \text{expand} \times \text{fully-connected}\,.
\end{align*}
\paragraph{Version 1.0} This document aims to provide an introduction to the following flavours:
\begin{align*}
  \text{PyTorch}
  \times
  \begin{Bmatrix}
    \rvec
    \\
    \cvec
  \end{Bmatrix}
  \times
  \begin{Bmatrix}
    \text{MC (type-I Fisher)}
    \\
    \text{type-II Fisher}
    \\
    \text{empirical Fisher}
  \end{Bmatrix}
  \times
  \text{expand}
  \times
  \text{fully-connected}
\end{align*}
This allows us to (i) highlight challenges when translating math to code, (ii) pointing out various connections between the curvature matrices and KFAC, and (iii) produce a working, tested, version of the original KFAC paper with slight generalizations.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
