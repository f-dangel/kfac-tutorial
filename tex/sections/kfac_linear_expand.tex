\par{\textbf{Roadmap: }} This chapter introduces the KFAC approximation for weights (or combined weight and bias) of fully-connected layers (i.e. a \texttt{torch.nn.Linear}). 
Our discussion will heavily rely on regression settings with deep linear networks, i.e.\,MLPs that consist of dense layers without any non-linearities.
We think they are a great setup to understand the core approximations of KFAC, and verify them numerically through rigorous tests.
In the context of a broader nomenclature we focus on the so-called KFAC-expand approximation which corresponds to the seminal KFAC approximation proposed by~\citet{martens2015optimizing}. It was introduced for standard MLP settings where the linear layers do not exhibit weight sharing (and therefore no reductions over weight-shared axes).
In scenarios of weight-sharing such as in convolutional layers further nuances of the KFAC approximation arise. For more details and references, see the outlook (??) or \citet{eschenhagen2023kroneckerfactored}.

\begin{setup}[Linear layer inside a neural network]\label{setup:linear_layer}
  Consider a linear layer with weights $\mW \in \sR^{D_{\text{out}}\times D_{\text{in}}}$ and bias $\vb \in \sR^{D_{\text{out}}}$ inside a neural network.
  Further, assume the network's prediction feeds into a criterion function and we use a data set of $N$ points to compute an empirical risk (with a reduction factor $R$) whose curvature matrix $\mC(\vec \mW)$ we are interested in computing.

  For each datum $n$, the layer processes a vector input $x_n \in \sR^{D {\text{in}}}$ into an output vector $\vz_{n} \in sR^{D_{\text{out}}}$ where 
  $$ \vz_{n} = \mW \vx_{n} + \vb.$$
  Denote by $\mF_n \in \sR^{C}$ the network's prediction on datum $n$. 
  For each vector $\blacktriangle_{n,c} \in \sR^{C}$ backpropagated by KFAC, define the errors arriving at the layer's output as $\vg_{n,c} = (\jac^{\vec}_{\vz_{n}} \mF_n)^{\top} \blacktriangle_{n,c} \in \sR^{D_{\text{out}}}$. These vectors do not depend on the flattening scheme because the contracted dimensions are summed out.

  In the following derivations we focus on a curvature approximation for the weight matrix $\mW$, essentially treating the bias term of the linear layer independently.
  However, in practice, weight and bias are often concatenated to a single weight matrix $\tilde{\mW}$ by appending a constant input dimension to the input vectors:
  \begin{align*}
    \tilde{\mW} &= \begin{bmatrix} W & b \end{bmatrix} \in \sR^{D_{\text{out}} \times (D_{\text{int}} + 1)}, \\
    \tilde{\vx} &= \begin{bmatrix} x \\ 1 \end{bmatrix} \in \sR^{D_{\text{in}} + 1}
  \end{align*}
  This replacement would yield analog derivations to below for $\tilde{\mW}$ instead of $\mW$. 
\end{setup}

\begin{caveat} 
  In some cases the linear layer operates on a sequence of $S$ input vectors, transforming $(\vx_{n,1}, \dots, \vx_{n, S})$ into output vectors $(\vz_{n, 1}, \dots, \vz_{n, S})$.
  This is also called a liear layer with weight-sharing, and there is the important distinction whether it operates on the sequence independently or not, such as in the case of a convolutional layer.
  In the former case, we can resolve the sequence dimension $S$ into the batch dimension $N$. A more formal motivation can be found at the end of Section (??).
  For more details on the problems of dealing with more general weight-sharing see \citet{eschenhagen2023kroneckerfactored}.
\end{caveat}

\switchcolumn[1]
\codeblock{kfac/expand_Linear}
\switchcolumn[0]

\subsection{Derivation of KFAC-expand}

Let's consider the GGN for a linear layer with weights $\mW$, input vector $\vx$, and output vector $\vz = \mW \vx$ (suppressing layer indices for simplicity). Let's also assume $\cvec$-flattening, as usually done by the literature and make use of the output-parameter Jacobian of a linear layer:
\begin{align*}
  \ggn_{\mW} \gL_{\sD}
  =
    R
    \sum_n &\sum_c
    (\jac^{\cvec}_{\mW}\vz_n)^{\top}
    (\jac^{\cvec}_{\vz_n}\vf_n)^{\top}
    \blacktriangle_{n,c} \\
    \times
    &\blacktriangle_{n,c}^{\top}
    (\jac^{\cvec}_{\vz_n}\vf_n)
    (\jac^{\cvec}_{\mW}\vz_n)
\end{align*}
Now recall that the Jacobian of a linear layer's output w.r.t its weights is given by
$$ \jac^{\cvec}_{\mW}\vz_n = \left(\vx_n \otimes \mI_{D_{\text{out}}}\right)^{\top} $$
where $\vx_n \in \R^{D_\text{in}}$ is the input vector of the layer. Then, by the property of the Kronecker product, we have for every vector $\vb \in \sR^{D_{\text{in}}}$
\begin{align*} 
  (\vx_n \otimes \mI_{D_{\text{out}}}) \vb &= \text{vec}\left(\vx_n \vb^{\top}\mI_{D_{\text{out}}}\right) \\
  &= \text{vec}\left( \vx_n \vb^{\top} \right) \\
  &= \vx_n \otimes \vb
\end{align*}
By denoting the layer output gradient by $\vb_{n,c} \coloneq (\jac^{\cvec}_{\vz_n}\vf_n)^{\top} \blacktriangle_{n,c}$, we can then derive:
\begin{align*}
  \ggn_{\mW} \gL_{\sD}
  &= R
  \sum_n \sum_c
  (\jac^{\cvec}_{\mW}\vz_n)^{\top}
  \vb_{n,c} \vb_{n,c}^{\top}
  (\jac^{\cvec}_{\mW}\vz_n) \\
  &= R \sum_n \sum_c (\vx_n \otimes \mI_{D_{\text{out}}}) \vb_{n,c} \vb_{n,c}^{\top}
  (\vx_n \otimes \mI_{D_{\text{out}}})^{\top} \\
  &= R \sum_n \sum_c (\vx_n \otimes \mI_{D_{\text{out}}}) \vb_{n,c} \left((\vx_n \otimes \mI_{D_{\text{out}}}) \vb_{n,c} \right)^{\top} \\
  &= R \sum_n \sum_c \left( \vx_n \otimes \vb_{n,c} \right) 
  \left( \vx_n \otimes \vb_{n,c} \right)^{\top} \\
  &= R \sum_n \sum_c \left( \vx_n \otimes \vb_{n,c} \right) 
  \left( \vx_n^{\top} \otimes \vb_{n,c}^{\top} \right) \\
  &= R \sum_n \sum_c \left(\vx_n \vx_n^{\top}\right) \otimes \left(\vb_{n,c} \vb_{n,c}^{\top}\right) \\
  &\approx \left( R \sum_n \vx_n \vx_n^{\top} \right) \otimes \left( \frac{1}{N} \sum_n \sum_c \vb_{n,c} \vb_{n,c}^{\top} \right).
\end{align*}

Instead of summing Kronecker products, we first sum the individual factors, then take their Kronecker product and divide by the number of summands.
The scaling is by convention absorbed into the gradient-based Kronecker factor. 
This strategy is the central approximation of layer-wise KFAC, it is also called KFAC's expectation approximation - for more formal motivation see Section ??.  
Note that we absorbed the reduction factor of the loss into the input-based factor.
The order of Kronecker factors changes depending on the flattening scheme, since the order of the Kronecker structure in the layer Jacobian changes. 
For the $\rvec$-flattening scheme the output-param jacobian is given by 
$$ \jac^{\cvec}_{\mW}\vz_n = \left(\vx_n \otimes \mI_{D_{\text{out}}}\right)^{\top}\,$$
which leads to a change of order of the Kronecker factors $\mA$ and $\mB$. 

\begin{definition}[KFAC-expand for a linear layer, \Cref{kfac/expand_Linear}]\label{def:kfac_expand_linear}
  Consider a linear layer inside a neural network from \Cref{setup:linear_layer}.
  The KFAC-expand approximation of a curvature matrix w.r.t.\,the layer's weights is given by
  \begin{subequations}\label{eq:kfac_expand_linear}
    \begin{align}
      \begin{split}
        &\kfac_{\text{exp}}(\vec \mW) \approx \mC(\vec \mW)
        \\
        &\quad=
          \begin{cases}
            \mA_{\text{exp}} \otimes \mB_{\text{exp}} & \vec = \cvec
            \\
            \mB_{\text{exp}} \otimes \mA_{\text{exp}} & \vec = \rvec
          \end{cases}\,,
      \end{split}
    \end{align}
    with the input-based factor $\mA_{\text{exp}} \in \sR^{D_{\text{in}} \times D_{\text{in}}}$
    \begin{align}
      \mA_{\text{exp}} &= R \sum_{n=1}^N \vx_{n} \vx_{n}^{\top}
                         \intertext{and the output-gradient-based factor $\mB_{\text{exp}} \in \sR^{D_{\text{out}} \times D_{\text{out}}}$}
                         \mB_{\text{exp}} &= \frac{1}{N}\sum_{n=1}^N \sum_c \vg_{n,c} \vg_{n,c}^{\top}\,.
    \end{align}
  \end{subequations}

  Here $\vg_{n,c}$ represents the general backpropagated vector of a curvature matrix at layer output $\vz$ given by $$\vg_{n,s,c} = (\jac_{\vz_{n,s}}^{\vec}\mF_n)^{\top} (\vec \blacktriangle_{n,c})$$. 
\end{definition}

\subsection{Algebraic Structure of a Curvature Matrix}
In the previous section we have seen that the curvature matrix at a linear layer is a sum of Kronecker products. 
For approximating this sum with a single Kronecker product, we followed the convention to carry out the sum over $n$ before taking the Kronecker product, which is summarized in the following definition:

\begin{definition}[KFAC's expectation approximation]
  Consider two sets of $N$ vectors $\{\vx_{n}\}_{n=1}^N$ and $\{\vg_{n}\}_{n=1}^N$ where each $\vx_n$ is input-based and each $\vg_n$ is output-gradient-based.
  KFAC approximates the summed Kronecker products of the self-outer products $\vx_n \vx_n^{\top} \otimes \vg_n \vg_n^{\top}$ as a single Kronecker products using the following expectation approximation:
  \begin{subequations}\label{eq:expectation_approximation}
    \begin{align}
      \begin{split}
        &\sum_{n=1}^{\textcolor{VectorOrange}{N}} \vx_n\vx_n^{\top} \otimes \vg_n \vg_n^{\top}
        \\
        &\qquad\approx
          \left( \sum_{n=1}^N \vx_n \vx_n^{\top} \right)
          \otimes
          \frac{
          \left( \textcolor{VectorOrange}{\sum_n} \vg_n \vg_n^{\top} \right)
          }{\textcolor{VectorOrange}{N}}
      \end{split}
      \intertext{In words, we sum the outer products on the left and right of the Kronecker product independently and divide by the number of outer products in the term containing the gradient vectors.
      In expectation notation (expectation meaning averaging over all values of $n$, i.e.\,$\E_n[\bullet_n] = \nicefrac{1}{N} \sum_{n=1}^N \bullet_n$), we can write this approximation more conveniently as}
      \begin{split}
        &\E_n \left[ \vx_n \vx_n^{\top} \otimes \vg_n \vg_n^{\top} \right]
        \\
        &\qquad\approx
          \E_n \left[ \vx_n \vx_n^{\top} \right]
          \otimes
          \E_n \left[ \vg_n \vg_n^{\top} \right]\,,
      \end{split}
    \end{align}
  \end{subequations}
  i.e.\,`pull' the expectation into the Kronecker factors.
  If $\vx$ and $\vg$ were independent, this approximation would be exact.
\end{definition}

A motivation for this approximation is that it arises under certain assumption as the optimal one.

\begin{example}[Justification for KFAC's expectation approximation]
  KFAC's expectation approximation can be derived from an optimality condition under special assumptions introduced to preserve a Kronecker structure.

  Assume we are given the expression
  \begin{align*}
    \mC
    &=
      R \sum_{n=1}^N
      (\vx_n \otimes \mI_{D_{\text{out}}})
      \vg_n \vg_n^{\top}
      (\vx_n^{\top} \otimes \mI_{D_{\text{out}}})
  \end{align*}
  with $\vx_n \in \sR^{D_{\text{in}}}$ and $\vg_n \in \sR^{D_{\text{out}}}$ and our goal is to approximate $\mC$ through a single Kronecker product.
  Let's first make this expression more compact by absorbing the summation over data points in to matrix multiplication,
  \begin{align*}
    \mC =&
    R (\mX \otimes \mI_{D_{\text{out}}})
    \cvec(\mG) \\
    &\cvec(\mG)^{\top}(\mX^{\top} \otimes \mI_{D_{\text{out}}})\,
  \end{align*}
  where we simply stack the vectors $\{ \vx_n \}$ and $\{\vg_n \}$ into matrices
  \begin{align*}
    \mX
    &=
      \begin{pmatrix}
        \vx_1 & \vx_2 & \ldots & \vx_N
      \end{pmatrix}
      \in \sR^{D_{\text{in}} \times N}\,,
    \\
    \mG
    &=
      \begin{pmatrix}
        \vg_1 & \vg_2 & \ldots & \vg_N
      \end{pmatrix}
      \in \sR^{D_{\text{out}} \times N}\,.
  \end{align*}
  Looking at the expression for $\mC$, we realize that an easy way to achieve a single Kronecker product is if we could somehow approximate $\cvec(\mG) \cvec(\mG)^{\top} \approx \mI_{D_{\text{in}}} \otimes \mB$ with $\mB \in \sR^{D_{\text{out}}\times D_{\text{out}}}$, because then we could write
  \begin{align*}
    \mC
    &\approx
      R (\mX \otimes \mI_{D_{\text{out}}})
      (\mI_{D_{\text{in}}} \otimes \mB)
      (\mX^{\top} \otimes \mI_{D_{\text{out}}})
    \\
    &=
      R \mX \mX^{\top} \otimes \mB
      =
      \mA \otimes \mB\,,
  \end{align*}
  which is what we would like to achieve.

  How do we find $\mB$? By minimizing the squared Frobenius norm residual
  \begin{align*}
    &\argmin_{\mB}
      \left\lVert
      \cvec(\mG) \cvec(\mG)^{\top} - \mI_{D_{\text{in}}} \otimes \mB
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \left\lVert
      \scalebox{0.75}{%
      $
      \begin{pmatrix}
        \vg_1 \vg_1^{\top} -\mB & \vg_1 \vg_2^{\top} & \dots & \vg_1 \vg_N^{\top} \\
        \vg_2 \vg_1^{\top} & \vg_2 \vg_2^{\top} - \mB & \ddots & \vg_2 \vg_N^{\top} \\
        \vdots & \ddots & \ddots & \vdots \\
        \vg_N \vg_1^{\top} & \vg_N \vg_2^{\top} & \dots & \vg_N \vg_N^{\top} - \mB \\
      \end{pmatrix}
      $
      }
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \left\lVert
      \scalebox{0.75}{%
      $
      \begin{pmatrix}
        \vg_1 \vg_1^{\top} - \mB & \vzero & \dots & \vzero \\
        \vzero & \vg_2 \vg_2^{\top} - \mB & \ddots & \vdots \\
        \vdots & \ddots & \ddots & \vzero \\
        \vzero & \hdots & \vzero & \vg_N \vg_N^{\top} - \mB \\
      \end{pmatrix}
      $
      }
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \sum_{n=1}^N
      \left\lVert
      \vg_n \vg_n^{\top} - \mB
      \right\rVert_2^2
    \\
    &=
      \argmin_{\mB}
      \sum_{n=1}^N
      \sum_{i,j = 1}^{D_{\text{out}}}
      \left(
      [\vg_n]_i [\vg_n]_j - [\mB]_{i,j}
      \right)^2 \,.
  \end{align*}
  Taking the derivative w.r.t.\,$\mB$ and setting it to zero yields
  \begin{align*}
    &\frac{\partial \left(
      \sum_{n=1}^N
      \sum_{i,j = 1}^{D_{\text{out}}}
      \left(
      [\vg_n]_i [\vg_n]_j - [\mB]_{i,j}
      \right)^2
      \right)
      }{
      \partial [\mB]_{k,l}
      }
    \\
    &=
      2 \sum_{n=1}^N
      \left(
      [\mB]_{k,l} -  [\vg_n]_k [\vg_n]_l
      \right)
    \\
    &=
      2 \left( N[\mB]_{k,l} - \sum_{n=1}^N [\vg_n]_k [\vg_n]_l \right)
      \stackrel{!}{=} 0 \qquad\forall k,l
    \\
    &\implies
      \mB = \frac{1}{N} \sum_{n=1}^N \vg_n \vg_n^{\top}
      = \frac{1}{N} \mG \mG^{\top}\,.
  \end{align*}
  Hence, if we `model' the gradient covariance as block-diagonal matrix with identical blocks $\mB$, the `best' choice is the average of gradient outer products, just like in KFAC,
  \begin{align*}
    \mC
    \approx
    \left(R \sum_n \vx_n \vx_n^\top \right)
    \otimes
    \left(\frac{1}{N} \sum_{n'} \vg_{n'} \vg_{n'}^\top \right)\,.
  \end{align*}
\end{example}

Similarily, we ca motivate flattening an additional sequence dimension in to the batch dimension, if no additional reduction along any axes occurs in the neural network. 
To do so, assume a sequence of input vectors as a matrix-valued input $\mX_n = (\vx_{n, 1}, \dots, \vx_{n, s})$ to the linear layer, a corresponding sequence of output vectors as a matrix $\mZ_n = \left( \vz_{n, 1}, \dots, \vz_{n,s} \right)$ and for any type of backprograted vectors a matrix $\mG_{n, c} = (\vg_{n, c, 1}, \dots, \vg_{n, c, s})$. The curvature matrix is then given by
\begin{align*}
  &\mC(\cvec \mW)
  \\
  &\quad\quad=
    R
    \sum_n \sum_c \sum_s
    (\jac^{\cvec}_{\mW}\vz_{n,s})^{\top}\cvec\vg_{n,c,s}\\
 &\quad(\cvec\vg_{n,c,s})^{\top}
   (\jac^{\cvec}_{\mW}\vg_{n,c,s}) \\
 &\quad=
   R
   \sum_n \sum_c (\mX_n \otimes \mI_{\vz_n})
 \\
 &\quad\quad
   \cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top}
   (\mX_n^{\top} \otimes \mI_{\vz_n}).
\end{align*}
Note that the matrix $\vec\mG_{n,c} (\vec\mG_{n,c})^{\top}$ is $S D_{\text{out}} \times S D_{\text{out}}$ and does not simplify into a single Kronecker product! 
Similar to Example ??, we would like to find an approximation $\cvec\mG_{n,c} (\cvec\mG_{n,c})^{\top} \approx \mK \otimes \mI_{D_\text{out}}$ to simplify the expression into Kronecker structure.
With the same arguments, it is easy to show that
\begin{align*}
  \argmin_{\mK}
  \left\lVert
  \cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top} - \mK \otimes \mI_{D_{\text{out}}}
  \right\rVert_2 \\
  =
  \frac{1}{S} \mG_{n,c} \mG_{n,c}^{\top}.
\end{align*}
i.e.\,we average the diagonal blocks of size $S \times S$, which simplifies the curvature approximation to 
\begin{align*}
  \mC&(\cvec\mW)
  \approx
  R \sum_n \sum_c
  \mZ_n \mZ_n^{\top} \otimes \frac{1}{S} \mG_{n,c} \mG_{n,c}^{\top}
  \\
  &= R \sum_n \sum_s \vz_{n,s} \vz_{n,s}^{\top} \otimes \sum_{s'} \sum_c \frac{1}{S} \vg_{n,s',c} \vg_{n,s',c}^{\top}
  \intertext{Applying then the expectation approximation from Definition ??, yields}
  &\approx
  \left(R \sum_n \sum_s \vz_{n,s} \vz_{n,s}^{\top}\right)
  \\
  &\quad\otimes \left( \frac{\textcolor{VectorOrange}{\sum_n} \textcolor{VectorTeal}{\sum_s}\sum_c \vg_{n,s,c} \vg_{n,s,c}^{\top}}{\textcolor{VectorOrange}{N} \textcolor{VectorTeal}{S}} \right)
\end{align*}
From this equation we can see that the shared dimension (highlighted in teal) is treated exactly like a batch dimension (highlighted in orange) in KFAC's expectation approximation.
% \color{\red Some observations on the matrix $\cvec \mG_{n,c} (\cvec \mG_{n,c})^{\top}$ while writing this: If the shared dimension is processed independently, then this matrix is block diagonal.
% But each $S \times S$ block is different.
% If the shared dimension is processed independently, and all layers are linear, then the matrix is block-diagonal and all $S \times S$ blocks on the diagonal are identical.
% This is the next test case for KFAC.}

\subsection{Tests}

Ideally we want to test a KFAC implementation against the full curvature for smaller models, which raises the question:  When does the approximation in \Cref{eq:expectation_approximation} become exact?
There are two cases, one of which is relatively obvious. 
Generally speaking, the approximation becomes exact whenever one of $\{\va_n\}$or $\{\vg_n\}$ are data-independent, i.e.\,do not depend on $n$.

\switchcolumn[1]*
\codeblock{kfac_tests/expand_mlp_batch_size_1}
\switchcolumn[0]

\subsubsection{Test Case: Data Set Size 1}
One obvious case is whenever the summation over $n$ disappears, i.e.\,our data set contains only a single data point (or $N$ identical data).

\begin{test}[KFAC-expand for linear layers in an MLP (no weight sharing), one data point, \Cref{kfac_tests/expand_mlp_batch_size_1}]\label{test:kfac_expand_linear_no_weight_sharing_batch_size_1}
  Consider a multi-layer perceptron
  \begin{align*}
    f = \phi^{(L)} \circ f^{(L)} \circ \ldots \circ \phi^{(1)} \circ f^{(1)}
  \end{align*}
  that processes a vector-valued input $\vx$ through a sequence of layers, each of which consists of a dense layer $f^{(l)}$ and a pointwise activation $\phi^{(l)}$.
  Let $\mW^{(l)}$, $\vb^{(l)}$, and $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$ denote the weight, bias, and combined parameters of a dense layer $f^{(l)}$.
  Further, assume the data set consists only of a single point, $\sD = \{ (\vx, \vy) \}$, and a criterion function $c = - \log r$ which can be interpreted as negative log likelihood.
  Then, KFAC-expand becomes exact in the following limits for $l = 1, \dots, L$, some flattening scheme $\vec$, and also when swapping the combined weights with only the weight matrix:
  \begin{itemize}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec\tilde{\mW}^{(l)}) = \ggn^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec\tilde{\mW}^{(l)})
      \\
      = \ggn^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \item KFAC-empirical equals the EF
    \begin{align*}
      \kfac^{\text{E}}_{\text{exp}}(\vec\tilde{\mW}^{(l)}) = \ef^{\vec}_{\tilde{\mW}^{(l)}}\gL_{\sD}
    \end{align*}
  \end{itemize}
\end{test}
This test is useful for checking functionality related to the criterion function, i.e.\,the backpropagated vectors $\blacktriangle_{n,c}$
It can further be generalized: We can replace the activation functions with arbitrary layers (e.g.\,reshape, convolution, pooling, normalization, \dots) as long as the inputs processed by the dense layers remain vector-shape.
These changes do not alter the structure of the GGN/Fisher/EF for the linear layers.

\switchcolumn[1]*
\codeblock{kfac_tests/input_only_last_layer_regression}
\switchcolumn[0]

\subsubsection{Test Case: Last layer FOOF}


\begin{test}[Last-layer FOOF for a linear layer without weight sharing~\cite{petersen2023isaac}, \Cref{kfac_tests/input_only_last_layer_regression}]
  Consider the MLP from \Cref{test:kfac_expand_linear_no_weight_sharing_batch_size_1}, but set $\phi^{(L)} = \mathrm{id}$ to identity, $c$ to square loss, and allow for a data set of arbitrary size $N$.
  Then we have the following exact relations between curvature matrices and KFAC flavours for the last layer:
  \begin{itemize}
  \item KFAC-expand-input-only equals the GGN
    \begin{align*}
      \kfac^{\text{IO}}_{\text{exp}}(\vec\tilde{\mW}^{(L)})
      &=
        \ggn^{\vec}_{\tilde{\mW}^{(L)}}\gL_{\sD}
        % \\
        % &=
        % \mA \otimes \mI
    \end{align*}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec\tilde{\mW}^{(L)})
      &=
        \ggn^{\vec}_{\tilde{\mW}^{(L)}}\gL_{\sD}.
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec\tilde{\mW}^{(L)})
      &=
        \ggn^{\vec}_{\tilde{\mW}^{(L)}}\gL_{\sD}.
    \end{align*}
  \item Importantly, KFAC-empirical does \emph{not} equal the empirical Fisher.
    \begin{align*}
      \kfac^{\text{E}}_{\text{exp}}(\vec\tilde{\mW}^{(L)})
      &\neq
        \ef^{\vec}_{\tilde{\mW}^{(L)}}\gL_{\sD}.
    \end{align*}
    This is because its backpropagated vectors $\blacktriangle_{n,c}$ depend on the datum $n$, while the other two flavours use vectors that do not depend on $n$.
  \end{itemize}
\end{test}

\switchcolumn[1]*
\codeblock{kfac_tests/expand_deep_linear_regression}
\switchcolumn[0]

\subsubsection{Test Case: Regression with Deep Linear Networks}

The more subtle and interesting testing scenario where the KFAC approximation becomes exact is for deep linear networks and square loss.
In this case $\vg_{n,c}$ does not depend on $n$.
To see this, let's look at the definition of $\vg_{n,c}$ which depends on the backpropagated vector $\blacktriangle_{n,c}$ and the Jacobian of the prediction w.r.t.\,the layer's output $\jac_{\vz_n} \vf_n$.
For square loss, $\blacktriangle_{n,c}$ often does not depend on $n$: For type-2, it is simply a column of the identity matrix, and for MC, it is simply a normally distributed random number.
If we further restrict all layers of our neural network to be linear layers all prediction-intermediate Jacobians are constant and therefore independent of $n$.
This implies the following limit in which KFAC becomes exact (first proposed in \cite{bernacchia2018exact}).

\begin{test}[KFAC-expand for regression with deep linear networks without weight sharing, \Cref{kfac_tests/expand_deep_linear_regression}]
  Consider a deep linear network consisting of $L$ dense layers
  \begin{align*}
    f = f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)}
  \end{align*}
  with weights $\mW^{(l)}$, bias $\vb^{(l)}$, and combined parameters $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$.
  Assume the network processes a vector-valued input per datum and consider regression with square loss on an arbitrary data set $\sD = \{ (\vx_n, \vy_n) \mid n = 1, \dots, N \}$.
  Then, we have the following equalities for $l = 1, \dots, L$ and a flattening scheme $\vec$ of our choice:
  \begin{itemize}
  \item KFAC-expand-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      =
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}
  \item KFAC-expand-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}=M}_{\text{exp}}(\vec \tilde{\mW}^{(l)})
      \\=
      \ggn^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}
    \end{align*}

  \end{itemize}
  Note that KFAC-expand-empirical does \emph{not} equal the empirical Fisher,
  \begin{align*}
    \kfac_{\text{exp}}^{\text{E}}(\vec \tilde{\mW}^{(l)})
    \neq
    \ef^{\vec}_{\tilde{\mW}^{(l)}} \gL_{\sD}\,,
  \end{align*}
  because its backpropagated vector $\blacktriangle_{n,c}$ depends on $\vf_n$ and therefore on $n$.
\end{test}
We can generalize this test further and allow for arbitrary operations that operate on an input in a linear fashion, e.g. convolutions, reshapes, padding, average pooling, as long as the input to a dense layer remains vector-valued.
We cannot pick non-linear layers like ReLU, sigmoid, max-pooling, or normalization, as their output-input Jacobian is input-dependent.

Note, that the test also allows for an additional sequence dimension $S$ that is treated independently by the network.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
