So far, we considered minimizing an empirical risk over a data set given an arbitrary criterion function $c$.
Here, we take a step back and first describe empirical risk minimization as an approximation of minimizing the intractable population risk.
This perspective connects to a probabilistic interpretation of empirical risk minimization as maximum likelihood estimation that will occur throughout the tutorial, \eg when we define the Fisher information matrix as curvature matrix (\cref{subsec:curvature-matrices}).

\paragraph{Empirical risk as expectation.} Recall the empirical risk from \cref{eq:empirical_risk} which we minimize during training,
\begin{align*}
  \min_{\vtheta} \gL_{\sD}(\vtheta) = \min_{\vtheta} R \sum_{n=1}^N \ell_n(\vtheta)\,.
\end{align*}
Why is it called an `empirical' risk?
Because it can be expressed as expectation over an empirical distribution in the following sense:

Assume there exists a data-generating process $p_{\text{data}}(\rvx, \rvy)$ over input-target pairs.
Ideally, we want to minimize the risk over that distribution,
\begin{align*}
  \argmin_{\vtheta} \E_{(\vx, \vy) \sim p_{\text{data}}(\rvx, \rvy)}[c(f(\vx, \vtheta), \vy)]\,.
\end{align*}
However, $p_{\text{data}}$ is intractable.
Therefore, we draw a finite collection of samples into a data set
\begin{align*}
  \sD = \{ (\vx_n, \vy_n) \mid (\vx_n, \vy_n) \stackrel{\text{i.i.d.}}{\sim} p_{\text{data}}(\vx, \vy) \}\,.
\end{align*}
Then, we can replace the intractable data-generating process $p_{\text{data}}$ with the tractable empirical distribution $p_{\sD}(\rvx, \rvy)$ implied by the data.
It consists of a uniformly weighted sum of delta peaks around the collected data points,
\begin{align*}
  p_{\sD}(\rvx, \rvy) = \frac{1}{N} \sum_{n=1}^N \delta(\rvx - \vx_n) \delta(\rvy - \vy_n)\,.
\end{align*}
This turns risk minimization into a tractable task:
\begin{align*}
  & \argmin_{\vtheta} \E_{(\vx, \vy) \sim \colored{p_{\text{data}}(\rvx, \rvy)}}[c(f(\vx, \vtheta), \vy)]
  \\
  \approx & \argmin_{\vtheta} \E_{(\vx, \vy) \sim \colored{p_{\sD}(\rvx, \rvy)}}[c(f(\vx, \vtheta), \vy)].
  \\
  \intertext{By writing out the expectation, we obtain}
  =       & \argmin_{\vtheta} \frac{1}{N} \sum_n c(f(\vx_n, \vtheta), \vy_n).
  \\
  \intertext{Note that the minimized objective is the empirical risk in \cref{eq:empirical_risk}
  scaled by $\nicefrac{1}{NR}$.
  However, we can arbitrarily scale objectives without changing the
  location of their minima.
  Hence, the above is equivalent to minimizing the empirical risk
  }
  =& \argmin_{\vtheta} \gL_{\sD}(\vtheta)\,.
\end{align*}
So now we have shown that empirical risk minimization is minimizing a (scaled) expectation of the criterion $c$ over an empirical density $p_{\sD}(\rvx, \rvy)$.

\paragraph{The neural net parameterizes a likelihood.}
Let's approach this from a probabilistic perspective now.
Assume we want to learn $p_{\text{data}}(\rvx, \rvy) = p_{\text{data}}(\rvy \mid \rvx) p_{\text{data}}(\vx)$ using a parameterized density of the form $p(\rvx, \rvy \mid \vtheta) = p(\rvy \mid \rvx, \vtheta) p_{\text{data}}(\rvx)$ where $p_{\text{data}}(\rvx) = \int p_{\text{data}}(\rvx, \rvy) \mathrm{d}\rvy$ is the marginal density of the input data.
Note that we model only the likelihood for the labels with parameters $\vtheta$.

One plausible approach to make $p$ resemble $p_{\text{data}}$ is to minimize their KL divergence,
\begin{align*}
  & \argmin_{\vtheta} \mathrm{KL}(p_{\text{data}}(\rvx, \rvy) \mid\mid p(\rvx, \rvy \mid \vtheta))\,.
  \\
  \intertext{We can simplify this expression by substituting the definition of the KL divergence and dropping terms that do not depend on $\vtheta$,}
  \Leftrightarrow & \argmin_{\vtheta} \E_{p_{\text{data}}(\rvx, \rvy)}[- \log( p(\rvx, \rvy \mid \vtheta))]\,.
  \\
  \intertext{This looks very similar to the expected risk from above.
  Next, let's factorize our model distribution using its conditional and marginal densities and drop $p_{\text{data}}(\rvx)$ as it does not depend on $\vtheta$,
  }
  \Leftrightarrow & \argmin_{\vtheta} \E_{p_{\text{data}}(\rvx, \rvy)}[- \log( p(\rvy \mid \rvx, \vtheta))]\,.
                    \intertext{To make this problem tractable, we need to replace the intractable data-generating process $p_{\text{data}}$ with the empirical distribution $p_{\sD}(\rvx, \rvy)$:}
                    \approx         & \argmin_{\vtheta} \E_{p_{\sD}(\rvx, \rvy)}[- \log( p(\rvy \mid \rvx, \vtheta))].
  \\
  \intertext{Writing out the expectation, we obtain}
  =               & \argmin_{\vtheta} \frac{1}{N} \sum_n - \log( p(\rvy = \vy_n \mid \rvx=\vx_n,\vtheta)).
  \\
  \intertext{To make this expression more similar to the empirical risk, we introduce a general scaling $R$ and change the likelihood's parameterization from $p(\rvy \mid \rvx, \vtheta)$ to $r(\rvy \mid f(\rvx, \vtheta))$ with a neural net $f$:}
  =               & \argmin_{\vtheta} R \sum_n - \log( r(\rvy = \vy_n \mid f(\vx_n,\vtheta)))
\end{align*}
This parameterization makes it clear that the neural network represents a conditional distribution over the labels given the inputs (and parameters).

TODO Below

\paragraph{The criterion is the negative log-likelihood.} We are now very close to writing down the explicit connection between empirical risk minimization and maximum likelihood estimation.
The last remaining step is to connect the model's likelihood $r(\rvy \mid f(\rvx, \vtheta))$ with the criterion function $c(f(\vx, \vtheta), \vy)$ from empirical risk minimization.

It turns out that empirical risk minimization with square loss (\cref{ex:square_loss}) corresponds to maximum likelihood estimation (or equivalently, negative log-likelihood minimization) of a Gaussian distribution over the labels (\cref{ex:square_loss_probabilistic}).

The following two examples show that empirical risk minimization with square  or softmax cross-entropy loss can be interpreted as maximum likelihood (or equivalently, minimum negative log-likelihood) estimation problem where the neural network parameterizes a Gaussian and a categorical distribution over the labels, respectively:

\switchcolumn[1]
\begin{example}[Probabilistic interpretation of the square loss]\label{ex:square_loss_probabilistic}
  For the square loss from \Cref{ex:square_loss}, we have that $c(\vf, \vy) = - \log( \mathrm{const.}
  \cdot \gN(\rvy \mid \vmu = f(\vx, \vtheta), \mSigma = \mI))$ where $\gN(\bullet | \vmu, \mSigma)$ is a multivariate Gaussian distribution with mean $\vmu \in \sR^C$ and positive definite covariance $\mSigma \in \sR^{C \times C}$,
  \begin{align*}
    \gN(\rvy \mid \vmu, \mSigma)
    =
    \frac{
    \exp\left( -\frac{1}{2} {(\rvy - \vmu)}^\top \mSigma^{-1} (\rvy - \vmu) \right)
    }{{(2\pi)}^{C/2} \sqrt{\det(\mSigma)}}\,.
  \end{align*}
  We can safely neglect the constant factor for the optimization problem and therefore identify
  \begin{align*}
    c                             & = \text{\texttt{MSELoss}}
    \\
                                  & \Leftrightarrow
    \\
    r(\rvy \mid f(\rvx, \vtheta)) & = \gN(\rvy \mid \vmu = f(\vx, \vtheta), \mSigma = \mI)\,
    \\
                                  & \Leftrightarrow
    \\
    p(\rvy \mid \rvx, \vtheta)    & = \gN(\rvy \mid \vmu = f(\vx, \vtheta), \mSigma = \mI)\,.
  \end{align*}
\end{example}

\begin{example}[Probabilistic interpretation of cross-entropy loss]\label{ex:cross_entropy_loss_probabilistic}
  For cross-entropy loss from \Cref{ex:cross_entropy_loss}, we have that $c(\vf, y) = - \log( \gC(\ry \mid \vsigma = \softmax(\vf) ))$ where $\gC(\bullet | \vsigma)$ is a categorical distribution over $\{1, \dots, C\}$ with probabilities $\vsigma \in \sR^C_{\ge 0}$ and $\vsigma^\top \vone = 1$,
  \begin{align*}
    \gC(\ry \mid \vsigma)
    =
    \prod_{c=1}^C [\vsigma]_c^{\delta_{\ry,c}}\,.
  \end{align*}
  Hence, we can identify
  \begin{align*}
    r(\ry \mid f(\rvx, \vtheta)) & = \gC(\ry \mid \vsigma = \softmax(f(\vx, \vtheta)))\,
    \\
                                 & \Leftrightarrow
    \\
    p(\ry \mid \rvx, \vtheta)    & = \gC(\ry \mid \vsigma = \softmax(f(\vx, \vtheta)))\,.
  \end{align*}
\end{example}
\switchcolumn[0]
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
