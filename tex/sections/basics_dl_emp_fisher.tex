Finally, we introduce the empirical Fisher information matrix. This matrix serves
as an approximation to the Type-I Fisher. Its most prominent appearance in the
optimization literature is through the Adam algorithm. Adam stores the moving
average of the loss' squared gradients to normalize the gradient values. This
corresponds to a diagonal approximation to preconditioning with the empirical
Fisher, defined below.

\begin{definition}[Empirical Fisher (\Cref{emp_fishers})]\label{def:emp_fisher}%
  The empirical matrix of the likelihood $\log r(\rvy \mid f(\vx_n, \vtheta))$,
  $\hat{\mF}(\vtheta) \in \sR^{D \times D}$ is defined as
  \begin{align*}
    & \hat{\mF}(\vtheta) \\
	& = \frac{1}{N} \sum_{n}
	\begin{aligned}[t]
	   & (-\nabla_{\vtheta} \log r(\rvy = \vy_n \mid \vf_n))        \\
	   & (-\nabla_{\vtheta} \log r(\rvy = \vy_n \mid \vf_n))^{\top} \\
	\end{aligned} \\
    & = \frac{1}{N} \sum_{n}
    \begin{aligned}[t]
       & \left(\jac_{\vtheta}\vf_n\right)^{\top}                  \\
       & (-\nabla_{\vf_n} \log r(\rvy = \vy_n \mid \vf_n))        \\
       & (-\nabla_{\vf_n} \log r(\rvy = \vy_n \mid \vf_n))^{\top} \\
       & \jac_{\vtheta}\vf_n.
    \end{aligned}
  \end{align*}
\end{definition}

\switchcolumn[1]*
\codeblock{basics/emp_fishers}
\codeblock{basics/emp_fisher_product}
\switchcolumn[0]

The subtle difference between the empirical and Type-I Fishers is that
$\mF^\text{I}(\vtheta)$ contains an \emph{expectation} over the gradient
outer product
\begin{align*}
  &(-\nabla_{\vtheta} \log r(\rvy = \vy_n \mid \vf_n)) \\
  &(-\nabla_{\vtheta} \log r(\rvy = \vy_n \mid \vf_n))^{\top},
\end{align*}
w.r.t.~the model's predictive distribution $r(\rvy \mid \vf_n)$,
whereas $\hat{\mF}(\vtheta)$ plugs in the \emph{ground-truth} labels $\vy_n$
into the gradient outer product. While the computation of the empirical Fisher
is more efficient than that Monte Carlo approximating the Type-I Fisher with
number of samples $M > 1$, this subtle difference
has severe implications for the utility of this matrix in optimization.
In particular,~\citet{kunstner2019limitations} show that preconditioning with
the empirical Fisher can have detrimental effects even in simple problems,
owing to its poor theoretical grounding. The empirical Fisher's success
in some settings (e.g., through Adam) can be attributed to its ability to attenuate
\emph{gradient noise}, not its properties as a curvature estimate.
