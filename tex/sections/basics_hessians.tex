\begin{definition}[Hessian of a vector-to-scalar function]
  TODO
\end{definition}

\begin{definition}[General Hessian of a tensor-to-scalar function]

\end{definition}

\begin{definition}[Hessian-vector products]

\end{definition}

\begin{definition}[$\cvec$-Hessian]

\end{definition}

\begin{definition}[$\rvec$-Hessian]

\end{definition}

\switchcolumn[1]*
\codeblock{hessian_ce_loss}

\switchcolumn[0]
\begin{example}[Softmax cross-entropy loss Hessian, \Cref{hessian_ce_loss}]
  Consider the softmax cross-entropy loss function between the vector-valued logits $\vx \in \sR^C$ and a class label $y \in \{1, \dots, C\}$:
  \begin{align*}
    \ell(\vx, y)
    &=
      -\log([p(\vx)]_y)\,.
  \end{align*}
  with $p(\vx) = \softmax(\vx) \in \sR^C$.
  Its Hessian w.r.t.\,$\vx$ is diagonal-minus-rank-one,
  \begin{align*}
    \hess_{\vx} \ell(\vx, y)
    =
    \diag(p(\vx)) - p(\vx) p(\vx)^\top\,.
  \end{align*}
\end{example}

\switchcolumn[1]*
\codeblock{hessian_mse_loss}

\switchcolumn[0]
\begin{example}[Square loss Hessian, \Cref{hessian_mse_loss}]
  Consider the square loss function between a vector-valued input $\vx \in \sR^C$ and its associated target $\vy \in \sR^C$:
  \begin{align*}
    \ell(\vx, \vy)
    &=
      \frac{1}{2}
      \left\lVert
      \vx - \vy
      \right\rVert^2\,.
  \end{align*}
  Its Hessian w.r.t.\,$\vx$ is proportional to the identity,
  \begin{align*}
    \hess_{\vx} \ell(\vx, \vy)
    =
    2 \mI_C\,.
  \end{align*}
\end{example}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
