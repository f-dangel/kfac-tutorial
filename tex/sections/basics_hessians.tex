Now that we have covered first-order derivatives, let's move on to second-order derivatives and develop the necessary concepts to understand KFAC, as well as their implementation.
Second-order derivatives are collected into an object called \emph{the Hessian}.
For our purposes, it will be sufficient to consider the Hessian of functions producing a scalar-valued output.
Let's start with the definition of the Hessian of a vector-to-scalar function.

\begin{setup}[Vector-to-scalar function]\label{setup:vector_to_scalar_function}
  Let $\vf: \sR^A \to \sR, \va \mapsto b = f(\va)$ be a vector-to-scalar function.
\end{setup}

\begin{definition}[Hessian of a vector-to-scalar function]\label{def:vector_hessian}
  The Hessian of a vector-to-scalar function $f$ from \Cref{setup:vector_to_scalar_function} is a matrix $\hess_{\va}b \in \sR^{A \times A}$ collecting the second-order partial derivatives of $f$ into a matrix with
  \begin{align*}
    [\hess_{\va}b]_{i,j}
     & =
    \frac{\partial^2 b}{\partial [\va]_i \partial [\va]_j}\,.
  \end{align*}
\end{definition}
This definition is limited to functions with vector-valued arguments. The extension to tensor-to-scalar functions is straightforward; however, it yields a tensor that is less convenient to work with in terms of notation:

\begin{setup}[Tensor-to-scalar function]\label{setup:hessians}
  Consider a tensor-to-scalar function $f: \sR^{A_1 \times \dots \times A_N} \to \sR, \tA \mapsto b = f(\tA)$ from a rank-$N$ tensor $\tA$ into a scalar $b$.
\end{setup}

\begin{definition}[General Hessian of a tensor-to-scalar function, \Cref{basics/hessians}]\label{def:general_hessian}
  The general Hessian of $f$ from \Cref{setup:hessians}, $\tH_{\tA}b \in \sR^{A_1 \times \dots \times A_N \times A_1 \times \dots \times A_N}$, collects the second-order partial derivatives of $f$ into a tensor with
  \begin{align*}
     & [\tH_{\tA}b]_{\colored{i_1, \dots, i_N}, \colored[VectorPink]{j_1, \dots, j_N}}
    \\
     & =
    \frac{\partial^2 b}{\partial [\tA]_{\colored{i_1, \dots, i_N}} \partial [\tA]_{\colored[VectorPink]{j_1, \dots, j_N}}}\,.
  \end{align*}
\end{definition}

\switchcolumn[1]*
\codeblock{basics/hessian_product}
\switchcolumn[0]

\paragraph{Hessian multiplication.}
Just like for Jacobians, the Hessian tensor is usually too large to be stored in memory.
Hence, one usually works with it implicitly through matrix-vector products, which can be done without computing the Hessian:

\begin{definition}[Hessian-vector products (HVPs), \Cref{basics/hessian_product}]\label{def:hvp}
  Given a tensor-to-scalar function $f$ from \Cref{setup:hessians} and a tensor $\tV \in \sR^{A_1 \times \dots \times A_N}$ in the input domain, the Hessian-vector product (HVP) $\tU$ of $\tV$ with $\tH_{\tA}b$ is the result of contraction with one of the Hessian's \colored{input indices},
  \begin{align*}
     & [\tU]_{i_1, \dots, i_N}
    \\
     & =
    \colored{\sum_{j_1, \dots, j_N}}
    [\tH_{\tA}b]_{i_1, \dots, i_N, \colored{j_1, \dots, j_N}} [\tV]_{\colored{j_1, \dots, j_N}}\,.
  \end{align*}
\end{definition}
For the vector case $N=1$, we have $\tV, \tA, \tH_{\tA}b \to \vv, \va, \hess_{\va}b$ and $\tU \to \vu = \hess_{\va} b$ as suggested by the name`Hessian-vector product'.

One way to multiply by the Hessian uses the so-called Pearlmutter trick~\cite{pearlmutter1994fast}.
It relies on the fact that multiplication with higher-order derivatives can be done by nested first-order differentiation.
Hence, multiplication with the Hessian can be done with two VJPs (\Cref{basics/hessian_product}).
In fact, this snippet implements a slightly more general Hessian that can handle differentiating twice \wrt \emph{different}, in contrast to the same, arguments.
It is not essential for understanding KFAC, and we will only use it to visualize curvature matrices in \Cref{subsec:curvature-matrices}.
In the context of KFAC, we do not care about these mixed second-order derivatives.

\switchcolumn[1]
\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[%
      % font=\scriptsize,%
      thick,
      box/.style = {rectangle, draw=black, rounded corners, fill=VectorGray!50},%%
    ]
    \node[box] (A) at (0,0) {$f: \tA \mapsto b = f(\tA)$};
    \node[box] (B) at (5.5,0) {$\tJ_{\tA}b$};
    \node[box, align=center] (C) at (0,-3) {%
      $f^{\vec} = f \circ \vec^{-1}$\\%
      $\vec(\tA) \coloneq \va \mapsto b$%
    };
    \node[box, align=center] (D) at (5.5,-3) {%
      $\hess_{\va} b$\\%
      $=$\\%
      $\mat(\tH_{\tA}b)$\\%
      $\coloneq$\\%
      $\hess^{\vec}_{\tA}b$%
    };
    \draw[-Stealth] (A.east) -- node[fill=white] {$\tJ$} (B.west);
    \draw[-Stealth] (A.south) -- node[fill=white] {flatten $f$} (C.north);
    \draw[-Stealth] (C.east) -- node[fill=white] {$\jac$} (D.west);
    \draw[-Stealth] (B.south) -- node[fill=white] {matricize $\tH_{\tA}b$} (D.north);
  \end{tikzpicture}
  \caption{\textbf{Flattening and taking the Hessian commute and lead to the same matricized Hessian.}
    $\vec$ denotes one of the flattening conventions from \Cref{def:cvec,def:rvec}.
    $\mat$ denotes matricization and involves two partial flattenings for row and column dimensions.}\label{fig:commutative-diagram-hessian}
\end{figure}
\switchcolumn[0]

\paragraph{Matricization:} For notational convenience, we will also define matricized versions of the general Hessian from \Cref{def:general_hessian}; the $\cvec$-, and $\rvec$-Hessian. Just like for the Jacobians, it does not matter whether we first flatten the function's input space then compute the Hessian, or compute the general Hessian then matricize it (\Cref{fig:commutative-diagram-hessian}).
The following definitions are consistent for both ways.

\switchcolumn[1]
\codeblock{basics/hessians}
\switchcolumn[0]

\begin{definition}[$\cvec$-Hessian, \Cref{basics/hessians}]\label{def:cvec_hessian}
  For a tensor-to-scalar function $f$ from \Cref{setup:hessians}, the $\cvec$-Hessian $\hess_{\tA}^{\cvec}b \in \sR^{A_N \cdots A_1 \times A_N \cdots A_1}$ results from flattening the input tensor with $\cvec$ and applying the Hessian from \Cref{def:vector_hessian},
  \begin{align*}
    [\hess^{\cvec}_{\tA}b]_{i, j}
     & =
    \frac{\partial^2 b}{\partial [\cvec(\tA)]_{i} \partial [\cvec(\tA)]_{j}}\,.
  \end{align*}
\end{definition}

\begin{definition}[$\rvec$-Hessian, \Cref{basics/hessians}]\label{def:rvec_hessian}
  For a tensor-to-scalar function $f$ from \Cref{setup:hessians}, the $\rvec$-Hessian $\hess_{\tA}^{\rvec}b \in \sR^{A_1 \cdots A_N \times A_1 \cdots A_N}$ results from flattening the input tensor with $\rvec$ and applying the Hessian from \Cref{def:vector_hessian},
  \begin{align*}
    [\hess^{\rvec}_{\tA}b]_{i, j}
     & =
    \frac{\partial^2 b}{\partial [\rvec(\tA)]_{i} \partial [\rvec(\tA)]_{j}}\,.
  \end{align*}
\end{definition}

Whenever we consider vector-to-scalar functions, both Hessians are identical, and we thus suppress the flattening scheme and write $\hess_{\va}b$.

\paragraph{Examples.}
Let's look at important Hessian examples we will return to later in the text.
We can use them to verify our Hessian implementation.

\switchcolumn[1]
\codeblock{basics/hessian_ce_loss}
\switchcolumn[0]

\begin{example}[Softmax cross-entropy loss Hessian, \Cref{basics/hessian_ce_loss}]\label{ex:hessian-crossentropyloss}
  Consider the softmax cross-entropy loss function between the vector-valued logits $\vf \in \sR^C$ and a class label $y \in \{1, \dots, C\}$ from \Cref{ex:cross_entropy_loss}:
  \begin{align*}
    c(\vf, y)
     & =
    -\log([\vsigma(\vf)]_y)\,.
  \end{align*}
  with $\vsigma(\vf) = \softmax(\vf) \in \sR^C$.
  Its Hessian \wrt $\vf$ is diagonal-minus-rank-one,
  \begin{align*}
    \hess_{\vf} c(\vf, y)
    =
    \diag(\vsigma) - \vsigma \vsigma^\top\,.
  \end{align*}
  See \eg~\citet{dangel2020modular} for a derivation.
\end{example}

\switchcolumn[1]
\codeblock{basics/hessian_mse_loss}

\switchcolumn[0]
\begin{example}[Square loss Hessian, \Cref{basics/hessian_mse_loss}]\label{ex:square_loss_hessian}
  Consider the square loss function between a vector-valued input $\vf \in \sR^C$ and its associated target $\vy \in \sR^C$ from \Cref{ex:square_loss}:
  \begin{align*}
    c(\vf, \vy)
     & =
    \frac{1}{2}\left\lVert
    \vf - \vy
    \right\rVert^2
    \\
     & =
    \frac{1}{2}(\vf - \vy)^{\top} \mI (\vf - \vy)\,.
  \end{align*}
  Its Hessian \wrt $\vf$ is the identity,
  \begin{align*}
    \hess_{\vf} \ell(\vf, \vy)
    =
    \mI_C\,.
  \end{align*}
\end{example}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
