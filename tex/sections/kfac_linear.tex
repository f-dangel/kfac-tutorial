The goal of this section should be to describe the KFAC approximation from~\cite{martens2015optimizing}

Let's consider the GGN for a linear layer with weights $\mW$, input vector $\vx$, and output vector $\vz = \mW \vx$ (suppressing layer indices for simplicity). Let's also assume $\cvec$-flattening, as usually done by the literature and make use of the output-parameter Jacobian of a linear layer (remember Example ??)
\begin{align*}
  \ggn_{\mW} \gL_{\sD}
  &=
    R
    \sum_n \sum_c
    (\jac^{\rvec}_{\mW}\vz_n)^{\top}
    (\jac^{\rvec}_{\vz_n}\vf_n)^{\top}
    \blacktriangle_{n,c}
    \blacktriangle_{n,c}^{\top}
    (\jac^{\rvec}_{\vz_n}\vf_n)
    (\jac^{\rvec}_{\mW}\vz_n)
  \\
  \shortintertext{
  (introduce $\vb_{n,c} \coloneq (\jac^{\rvec}_{\vz_n}\vf_n)^{\top} \blacktriangle_{n,c}$ for the layer output gradient)
  }
  &=
    R
    \sum_n \sum_c
    (\jac^{\rvec}_{\mW}\vz_n)^{\top}
    \vb_{n,c} \vb_{n,c}^{\top}
    (\jac^{\rvec}_{\mW}\vz_n)
    \shortintertext{Insert the linear layer's Jacobian}
  &=
    R
    \sum_n \sum_c
    (\vx_n \otimes \mI_{D_{\text{out}}})
    \vb_{n,c} \vb_{n,c}^{\top}
    (\vx_n^{\top} \otimes \mI_{D_{\text{out}}})
  \\
  &=
    R
    \sum_c
    \sum_n
    \vx_n \vx_n^{\top}
    \otimes
    \vb_{n,c} \vb_{n,c}^{\top}
\end{align*}

The expectation approximation in KFAC is the following:
\begin{align}
  \sum_n \va_n\va_n^{\top} \otimes \vb_n \vb_n^{\top}
  \approx
  \left( \sum_n \va_n \va_n^{\top} \right)
  \otimes
  \frac{
  \left( \sum_n \vb_n \vb_n^{\top} \right)
  }{N}
\end{align}
Instead of summing Kronecker products, we first sum the individual factors, then take their Kronecker product and divide by the number of summands. The scaling is by convention absorbed into the gradient-based Kronecker factor.

If we apply this approximation to the above, we obtain
\begin{align*}
  \kfac(\ggn_{\mW}\gL_{\sD})
  &\approx \ggn_{\mW}\gL_{\sD}
  \\
  &\coloneq
    \underbrace{
    \left(
    R
    \sum_n
    \vx_n \vx_n^{\top}
    \right)
    }_{\coloneq \mA}
    \otimes
    \underbrace{
    \frac{
    \left(
    \sum_n
    \sum_c
    \vb_{n,c} \vb_{n,c}^{\top}
    \right)
    }{N}
    }_{\coloneq \mB}
\end{align*}
Note that we absorbed the reduction factor of the loss into input-based factor.



\begin{test}[KFAC]
  Consider a deep linear network of depth $L$,
  \begin{align*}
    f(\vx) = (f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)})(\vx)\,,
  \end{align*}
  or
  \begin{align*}
    \vx^{(L)} = f(\vx^{(0)})
  \end{align*}
  via the intermediates
  \begin{align*}
    f^{(i)}:  \vx^{(i-1)} \mapsto \vx^{(i)} \mW^{(i)} \vx^{(i)} + \vb^{(i)}\,.
  \end{align*}
  Let $\tilde{\mW}^{(i)} = \begin{pmatrix} \mW^{(i)} & \vb^{(i)}\end{pmatrix}$ denote the concatenation of weights and biases of a layer $i$.

  Consider a data set $\sD = \left\{ (\vx_n, \vy_n) \mid n = 1, \dots N \right\}$ and its empirical risk under square loss,
  \begin{align*}
    \gL_{\sD}(\tilde{\mW}^{(1)}, \dots, \tilde{\mW}^{(L)})
    &=
      R \sum_{n=1}^N \ell(f(\vx_n), \vy_n)
    \\
    &=
      R \sum_{n=1}^N c(f(\vx_n), \vy_n)
    \\
    &=
      R \sum_{n=1}^N \left\lVert \vx_n^{(L)} - \vy_n  \right\rVert^2
  \end{align*}
  where $R$ is a constant (the so-called reduction factor, usually $1$ or $\nicefrac{1}{N}$).

  Under the function split $\ell = c \circ f$.

  Then, we have the following equalities:
  \begin{itemize}
  \item The per-layer GGN equals KFAC.
    \begin{align*}
      \ggn^{\rvec}_{\tilde{\mW}^{(i)}} \gL_{\sD}
      =
      \left( R \sum_{n=1}^{|\sD|} \vx^{(i-1)} {\vx^{(i-1)}}^{\top}  \right)
      \\
      \otimes
      \left(
      \sum_{n=1}^{|\sD|} \frac{\partial \ell_n}{\partial \vx^i}
      \left(
      \frac{\partial \ell_n}{\partial \vx^i}
      \right)^{\top}
      \right)
    \end{align*}
  \end{itemize}
\end{test}

\switchcolumn[1]
\codeblock{kfac/expand_Linear}
\switchcolumn[0]

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
