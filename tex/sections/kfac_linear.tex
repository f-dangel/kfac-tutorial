\paragraph{Roadmap:} This chapter introduces the KFAC approximation for weights (or combined weight and bias) of fully-connected layers (i.e.\,a \texttt{torch.nn.Linear}).
It comes in two flavours~\cite{eschenhagen2023kroneckerfactored}: KFAC-expand and KFAC-reduce; the names will become clear by the end.
Roughly speaking, they are motivated by the presence of weight sharing and reductions over weight-shared dimensions during a forward pass.

Our discussion will heavily rely on regression settings with deep linear networks, i.e.\,MLPs that consist of dense layers without any non-linearities.
We think they are a great setup to understand the core approximations of KFAC, and verify them numerically through rigorous tests.

We will start with the KFAC-expand approximation, which corresponds to the seminal KFAC approximation proposed by~\citet{martens2015optimizing}. It was introduced for standard MLP settings where the linear layers do not exhibit weight sharing (and therefore no reductions over weight-shared axes).
We will then show how to extend KFAC-expand to the presence of weight sharing.

The discussion of KFAC-expand naturally leads to an alternative setting where weight sharing dimensions are reduced during a forward pass; the so-called reduce setting.
For this setting, KFAC is not well motivated as it has no limit in which it becomes exact.
As a solution, we will explain the alternative KFAC-reduce approximation, recently introduced by~\citet{eschenhagen2023kroneckerfactored}.

\subsection{KFAC-expand}

Let's consider a good old MLP with $L$ layers,
\begin{align*}
  f = f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)} \,.
\end{align*}
Each layer $f^{(l)}$ is either a an elementwise activation function (such as ReLU, tanh, sigmoid), or a linear layer with weight $\mW^{(l)}$ and bias $\vb^{(l)}$ (and their combined parameter $\tilde{\mW}^{(l)} = \begin{pmatrix} \mW^{(l)} & \vb^{(l)} \end{pmatrix}$).
The MLP processes a vector-valued input $\vx_n$ into a vector-valued output $\vf_n = f(\vx_n, \vtheta)$.
Because all layers are dense or point-wise, all intermediates are also vector-valued.

We are interested in approximating the block of a curvature matrix $\mC(\vec \mW^{(l)})$ (with $\vec$ some flattening convention) of the form~\Cref{eq:curvature_matrix}, e.g.\,the empirical Fisher, MC-sampled Fisher, type-II Fisher/GGN. Our derivations use $\vec \mW^{(l)}$; we describe the modifications for $\vec \tilde{\mW}^{(l)}$ later.

To declutter notation, let's suppress the layer index and instead of $\vx_n^{(l)} = \mW^{(l)}\vx_n^{(l-1)} + \vb^{(l)}$ write $\vz_n = \mW \va_n + \vb$, using $\va_n$ for the layer input and $\vz_n$ its corresponding output. Because the literature usually adopts column-stacking as flattening convention, we will use $\cvec$.
Starting from \Cref{eq:curvature_matrix}, let us first split the Jacobian of the prediction w.r.t.\,the weight using the chain rule
$(\jac^{\cvec}_{\mW}\vf_n)^{\top} = (\jac^{\cvec}_{\mW}\vz_n)^{\top} (\jac^{\cvec}_{\vz_n}\vf_n)^{\top}$ and introduce the shortcut $\vg_{n,c} \coloneq (\jac^{\cvec}_{\vz_n}\vf_n)^{\top} \blacktriangle_{n,c}$ for the vectors backpropagated to the layer's output:
\begin{align*}
  &\mC(\cvec \mW)
  \\
  &\quad=
    R
    \sum_n \sum_c
    (\jac^{\cvec}_{\mW}\vz_n)^{\top}
    \vg_{n,c} \vg_{n,c}^{\top}
    (\jac^{\cvec}_{\mW}\vz_n)
    \shortintertext{After inserting the linear layer's output-weight Jacobian from \Cref{ex:weight_jacobians_linear_layer}, we get}
  &\quad=
    R
    \sum_n \sum_c
    (\va_n \otimes \mI_{\vz_n})
    \vg_{n,c} \vg_{n,c}^{\top}
    (\va_n^{\top} \otimes \mI_{\vz_n})
    \shortintertext{which simplifies to}
  &\quad=
    R
    \sum_c
    \sum_n
    \va_n \va_n^{\top}
    \otimes
    \vg_{n,c} \vg_{n,c}^{\top}\,.
\end{align*}
The curvature matrix is a sum of Kronecker products.
We want to approximate this with a single Kronecker product.

To do that, we need to agree on a convention to carry out the sum over $n$ before taking the Kronecker product. We will use the following convention:
\begin{align}\label{eq:expectation_approximation}
  \begin{split}
    &\sum_{n=1}^{\textcolor{VectorOrange}{N}} \va_n\va_n^{\top} \otimes \vg_n \vg_n^{\top}
    \\
    &\quad\approx
      \left( \sum_{n=1}^N \va_n \va_n^{\top} \right)
      \otimes
      \frac{
      \left( \textcolor{VectorOrange}{\sum_n} \vg_n \vg_n^{\top} \right)
      }{\textcolor{VectorOrange}{N}}
  \end{split}
\end{align}
In words, we sum the outer products on the left and right of the Kronecker product independently and divide by the number of outer products in the term containing the gradient vectors.

\switchcolumn[1]*
\codeblock{kfac/expand_Linear}
\switchcolumn[0]

If we apply \Cref{eq:expectation_approximation}, we obtain
\begin{definition}[KFAC-expand for a linear layer with vector-valued input, \Cref{kfac/expand_Linear}]
  Consider a linear layer with weight $\mW$ and bias $\vb$ inside a neural network that processes a vector-valued input $\va_n$ for each datum $n$ in to a vector-valued output $\vz_n = \mW \va_n + \vb$. The KFAC-expand approximation of a curvature matrix $\mC(\cvec \mW)$ in column-flattening convention is given by
  \begin{subequations}
    \begin{align}
      \kfac_{\text{exp}}(\cvec \mW)
      =
      \mA \otimes \mB
      \approx \mC(\cvec \mW)\,,
      \intertext{where}
      \mA = R \sum_n \va_n \va_n^{\top}\,,
      \quad
      \mB = \frac{\left(\sum_n \sum_c \vg_{n,c} \vg_{n,c}^{\top} \right)}{N}\,.
    \end{align}
  \end{subequations}
  The reduction factor from the loss function is absorbed into the input-based Kronecker factor (this is done by the scaffold in \Cref{kfac/scaffold}).

  If we treat weight and bias jointly, we need to make the substitutions
  \begin{align*}
    \mW \leftrightarrow \tilde{\mW}
    \quad
    \text{and}
    \quad
    \va_n \leftrightarrow \begin{pmatrix} \va_n \\ 1 \end{pmatrix}\,.
  \end{align*}
\end{definition}
We can work out the same steps to approximate the curvature matrix in row-stacking flattening, which simply leads swapping the two Kronecker factors,
\begin{align*}
  \kfac_{\text{exp}}(\rvec \mW)
  =
  \mB \otimes \mA
  \approx \mC(\rvec \mW)\,.
\end{align*}

\subsection{Sanity Check with Batch Size 1}

\subsection{Tests}
When does this approximation become exact?
Generally speaking, it becomes exact whenever one or both of the Kronecker factors in ??
does not depend on $n$.

\switchcolumn[1]*
\codeblock{kfac_tests/expand_mlp_batch_size_1}
\switchcolumn[0]

\paragraph{One datum:} One obvious case is whenever the summation over $n$ disappears, i.e.\,our data set contains only a single data point (or consists of $N$ identical data points).
\begin{test}[KFAC for linear layers in an MLP (no weight sharing), one data point, \Cref{kfac_tests/expand_mlp_batch_size_1}]
  Consider a multi-layer perceptron
  \begin{align*}
    f = \phi^{(L)} \circ f^{(L)} \circ \ldots \circ \phi^{(1)} \circ f^{(1)}
  \end{align*}
  that processes a vector-valued input $\vx$ through a sequence of layers, each of which consists of a dense layer $f^{(i)}$ and a pointwise activation $\phi^{(i)}$.
  Let $\mW^{(i)}$, $\vb^{(i)}$, and $\tilde{\mW}^{(i)} = \begin{pmatrix} \mW^{(i)} & \vb^{(i)} \end{pmatrix}$ denote the weight, bias, and combined parameters of a dense layer $f^{(i)}$.
  Further, assume the data set consists only of a single point, $\sD = \{ (\vx, \vy) \}$, and a criterion function $c = - \log r$ which can be interpreted as negative log likelihood.
  Then, KFAC becomes exact in the following limits for $i = 1, \dots, L$:
  \begin{itemize}
  \item KFAC-type-II equals the GGN
    \begin{align*}
      \kfac^{\text{II}}(\tilde{\mW}^{(i)}) = \ggn_{\tilde{\mW}^{(i)}}\gL_{\sD}
    \end{align*}
  \item KFAC-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}}_M(\tilde{\mW}^{(i)}) = \ggn_{\tilde{\mW}^{(i)}}\gL_{\sD}
    \end{align*}
  \item KFAC-empirical equals the EF
    \begin{align*}
      \kfac^{\text{E}}(\tilde{\mW}^{(i)}) = \ef_{\tilde{\mW}^{(i)}}\gL_{\sD}
    \end{align*}
  \end{itemize}
\end{test}
This test is useful for checking functionality related to the criterion function.
It can further be generalized: We can replace the activation functions with arbitrary layers (e.g.\,reshape, convolution, pooling, normalization, \dots) as long as the inputs processed by the dense layers remain vector-shape.
These changes do not alter the structure of the GGN/Fisher/EF for the linear layers.

\switchcolumn[1]*
\codeblock{kfac_tests/expand_deep_linear_regression}
\switchcolumn[0]

\paragraph{Constant prediction-layer-output Jacobian} The second, less obvious case is when $\vb_{n,c}$ does not depend on $n$.
Looking at the definition of $\vb_{n,c}$, it depends on the backpropagated vector $\blacktriangle_{n,c}$ and the Jacobian of the prediction w.r.t.\,the layer's output $\jac_{\vz_n} \vf_n$.
If we choose square loss, $\blacktriangle_{n,c}$ does not depend on $n$: For type-2, it is simply a column of the identity matrix, and for MC, it is simply a normally distributed random number.
If we further restrict all layers of our neural network to be linear w.r.t.\,their input, all prediction-intermediate Jacobian will be constant and therefore independent of $n$.
Such layers are e.g., dense layers, convolutions, reshapes, padding, average pooling.
We cannot pick non-linear layers like ReLU, sigmoid, max-pooling, or normalization, as their output-input Jacobian is input-dependent. This implies the following limit in which KFAC becomes exact (first proposed in \cite{bernacchia2018exact}).

\begin{test}[KFAC for linear layers in a deep linear MLP (no weight sharing) for regression, \Cref{kfac_tests/expand_deep_linear_regression}]
  Consider a deep linear network consisting of $L$ dense layers
  \begin{align*}
    f = f^{(L)} \circ f^{(L-1)} \circ \ldots \circ f^{(1)}
  \end{align*}
  with weights $\mW^{(i)}$, bias $\vb^{(i)}$, and combined parameters $\tilde{\mW}^{(i)} = \begin{pmatrix} \mW^{(i)} & \vb^{(i)} \end{pmatrix}$.
  The network processes a vector-valued input.
  Consider regression with square loss on an arbitrary data set $\sD = \{ (\vx_n, \vy_n) \mid n = 1, \dots, N \}$.
  Then, we have the following equalities for $i = 1, \dots, L$ and a flattening scheme $\vec$ of our choice:
  \begin{itemize}
  \item KFAC-type-II equals the GGN
    \begin{align*}
      \kfac(\vec \tilde{\mW}^{(i)}) = \ggn^{\vec}_{\tilde{\mW}^{(i)}} \gL_{\sD}
    \end{align*}
  \item KFAC-MC converges to the GGN
    \begin{align*}
      \lim_{M \to \infty} \kfac^{\text{MC}}_M(\vec \tilde{\mW}^{(i)}) = \ggn^{\vec}_{\tilde{\mW}^{(i)}} \gL_{\sD}
    \end{align*}

  \end{itemize}
  KFAC-empirical does \emph{not} equal the EF,
  \begin{align*}
    \kfac^{\text{E}}(\vec \tilde{\mW}^{(i)}) \neq \ef^{\vec}_{\tilde{\mW}^{(i)}} \gL_{\sD}\,,
  \end{align*}
  because the backpropagated vector depends on $\vf_n$ and therefore on $n$.
\end{test}

\switchcolumn[1]*
\codeblock{kfac_tests/input_only_last_layer_regression}
\switchcolumn[0]

\paragraph{Input-based KFAC (last layer)}~\cite{petersen2023isaac}

\subsection{How to handle weight sharing?}
What happens if the input to our neural network is not a single vector, but a collection of vectors (i.e.\,a matrix)? Assume a linear layer is processing a sequence of vectors $\mX =
\begin{pmatrix} \vx_1 & \dots & \vx_S \end{pmatrix}$ into a sequence of output vectors $\mZ =
\begin{pmatrix} \vz_1 & \dots & \vz_S \end{pmatrix}$ where each $\vz_i = \mW \vx_i$.
We say that these vectors \emph{share} the same weight.
If each shared dimension is processed independently, we can treat it just like a batch dimension.
This is the case if we use an MLP that consists of dense layers and element-wise activation functions.
The test in \Cref{kfac_tests/expand_deep_linear_regression} includes such a scenario where $S > 1$.
Note however that in practise we want to combine the features in a sequence (e.g.\,as done in the attention layer).
But for a simple MLP, the shared axis is just treated like a batch axis, which motivates treating it as such in the KFAC-expand approximation.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
