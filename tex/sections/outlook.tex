This contains a list of further resources and topics that might be discussed in later versions:

\paragraph{Other layers.}

\paragraph{Eigenvalue correction.} TODO

\paragraph{Sequence-valued data.}
How to define Fisher matrices for neural networks with $>2$d outputs?
Mention how \cite{grosse2023studying} do it.
Touch upon the point that there might be other choices.
Say that we believe this is not fully understood and might need some additional research.

\paragraph{Generalizing the implementation by accessing layer inputs/outputs via a graph tracing mechanism.}
We use PyTorch's hook mechanism to intercept the forward pass and obtain the layer inputs (required for KFAC's input-based Kronecker factor) and outputs (required for KFAC's grad-output-based Kronecker factors).
This limits the supported neural networks to models constructed with PyTorch's modular \texttt{nn} sub-module.
This is okay because most architectures used in practise are indead implemented with \texttt{torch.nn.Module}s.
An alternative way would be to symbolically trace the neural network's compute graph, \eg using PyTorch's \texttt{fx} library \cite{reed2022torch}, and modify it to return the necessary layer inputs and outputs in addition to the original return values.
It seems that this is how KFAC-JAX \cite{botev2022kfac-jax} is implemented.
This approach might be helpful, \eg to compute KFAC for physics-informed loss functions (see \citet{dangel2024kroneckerfactored} for details).
These losses still allow KFAC to be applied, but their computation cannot be expressed as simple forward pass through the network.
Instead, it involves multiple slightly different forward passes in Taylor arithmetic, for which the hook-based capturing mechanism breaks down, while a graph-tracing based approach would still be capable to identity all tensors that are processed by a single weight matrix.
However, symbolic tracing has its own limitations.
For instance, we cannot trace data-dependent control flow (think \texttt{if-else} conditions that depend on the value of intermediate tensors).
\texttt{torch.fx} focusses on ease of use at the cost of generality.
Overall, we think it would be interesting to look into this approach to implement KFAC, as it would allow to slightly generalize the supported function classes.

\paragraph{KFAC-reduce} becomes relevant in scenarios where weight sharing occurs, and the shared dimension is explicitly reduced (\eg, summed or averaged) during the forward pass. Unlike KFAC-expand, which treats shared dimensions as independent by flattening them into a batch dimension, KFAC-reduce adjusts for this reduction by aggregating inputs and gradients across the shared dimension before forming Kronecker factors.

This distinction is important for models such as convolutional networks, attention mechanisms, or architectures with sequence inputs, where reductions over spatial or temporal dimensions are common. KFAC-reduce addresses the failure of KFAC-expand to capture the correct curvature structure in such cases, ensuring a well-motivated approximation even when the shared dimension is collapsed during computation. For a detailed treatment and the derivation of KFAC-reduce, see \citet{eschenhagen2023kroneckerfactored}.

\paragraph{Plan for future versions}
\begin{itemize}
\item JAX version
\item KFAC-expand for convolutions \cite{grosse2016kroneckerfactored}
\item KFAC-reduce for convolutions \cite{eschenhagen2023kroneckerfactored}
\item KFAC for linear layers with weight sharing \cite{eschenhagen2023kroneckerfactored}
\item KFAC for RNNs \cite{martens2018kroneckerfactored}
\end{itemize}