\subsection{Empirical risk minimization \& Maximum Likelihood Estimation}

\subsubsection{Deep neural networks}

\subsubsection{Probabilistic interpretation}


\begin{align}
  \gL(\vtheta) &= \sum_{n=1}^N \ell(\vtheta, \vx_n, \vy_n)
  \\
               &=
                 \sum_{n=1}^N c(f(\vtheta, \vx_n), \vy_n)
\end{align}

\switchcolumn[0]

\blindtext

\switchcolumn[1]

\switchcolumn[0]* % sync

\blindtext

\subsection{Derivatives \& Automatic Differentiation}

\begin{caveat}
  In deep learning, we often work with matrices, or higher-dimensional tensors.
  We want to use matrix linear algebra expressions to avoid using heavy index notation.
  This can be achieved by flattening all tensors back into vectors and re-using definitions derivatives from the vector case.
  However, we must be careful when translating the results back to the tensor format, as the translation process depends on the flattening convention.
  Classically, the mathematical derivations prefer a \emph{different} flattening scheme than the one used in deep learning libraries.
\end{caveat}

\switchcolumn[0]*
\subsubsection{Flattening}
\input{sections/basics_flattening.tex}

\switchcolumn[0]*
\subsubsection{Jacobians, JVP, VJPs}
\input{sections/basics_jacobians.tex}

\switchcolumn[0]*
\subsubsection{Hessians, HVPs}
\input{sections/basics_hessians.tex}

\switchcolumn[0]*
\subsubsection{Partial Linearization, Generalized Gauss-Newtons, GGNVPs}
\input{sections/basics_linearization.tex}

\switchcolumn[0]*
\subsection{Curvature Matrices in Deep Learning}

The previous section introduced Jacobians, Hessians, partial linearizations and the resulting generalized Gauss-Newton (GGN) objects in the language of automatic differentiation for arbitrary functions.
Here, we will switch gears and quickly walk through how these matrices look like in the context of deep learning.

\paragraph{Parameter list/tuple format:} One new aspect we have to deal with is that ML libraries like PyTorch represent parameters as lists/tuples of variables.
We consider a neural network $f(\vtheta, x): \Theta \times \gX \to \gF$ with parameters in list/tuple-format,
\begin{align*}
  \vtheta = (\vtheta^{(1)}, \vtheta^{(2)}, \ldots, \vtheta^{(L)}),
\end{align*}
where each $\vtheta^{(i)}$ is an arbitrary tensor. To be able to use matrix expressions, we will often consider the concatenation of flattened vectors,
\begin{align*}
  \vec(\vtheta)
  =
  \begin{pmatrix}
    \vec(\vtheta^{(1)}) \\
    \vec(\vtheta^{(2)}) \\
    \vdots \\
    \vec(\vtheta^{(L)})
  \end{pmatrix}
  \in \sR^D
  \,,
\end{align*}
where $\vec \in \{ \rvec, \cvec \}$ is one of the previously described flattening operations.
This convention allows $\vec$ to handle parameters in list/tuple format, and is a simple generalization which simply applies the original definition to each element of the list/tuple and concatenates all flattened items.
However, in code we will still work with the list/tuple format.

\paragraph{Empirical risk:} We consider a data set $\sD = \{(\vx_n, \vy_n) \in \gX \times \gY \mid n = 1, \dots, N \}$ containing $N$ i.i.d.\,samples.
The inputs are processed by the neural network and the resulting predictions are scored with a criterion function $c: \gF \times \gY \to \sR$, such as mean-squared or softmax cross-entropy loss.
For a datum $n$, we define the per-datum loss as:
\begin{align*}
  (\ell_n: \Theta \to \sR) = (c_n \circ f_n: \Theta \to \gF \to \sR)
  \\
  \ell_n(\vtheta) = c_n(f_n(\vtheta))
\end{align*}
where $c_n(\bullet) \coloneq c(\bullet, \vy_n)$ and $f_n(\bullet) \coloneq f(\bullet, \vx_n)$.
We will often use the shorthands $c_n, f_n, \ell_n$ for the per-datum predictions, criteria, and losses.

\paragraph{Reduction factor:} The per-datum losses $\{\ell_1, \dots, \ell_N\}$ are accumulated into a single scalar which yields the empirical risk
\begin{align*}
  \gL_{\sD}: \Theta &\mapsto \sR
  \\
  \vtheta &\mapsto \gL_{\sD}(\vtheta) = R \sum_{n=1}^N \ell_n(\vtheta) = R \sum_{n=1}^N c_n(f_n(\vtheta))
\end{align*}
where $R$ is the reduction factor. Common values for $R$ are $\nicefrac{1}{N}, 1$ and $\nicefrac{1}{N \dim(\gF)}$.
For the purpose of this text, $\sD$ can be any collection of data points, e.g.\,the full data set or a mini-batch.

\subsubsection{The Hessian}\label{sec:basics_dl_hessian}
\input{sections/basics_dl_hessian.tex}

\subsubsection{The Generalized Gauss-Newton (GGN)}
Linearization

\subsubsection{The Fisher}
Probabilistic perspective

Explain type-1 versus type-2
\subsubsection{The Connection between GGN \& Fisher}
\subsubsection{The Empirical Fisher (EF)}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
