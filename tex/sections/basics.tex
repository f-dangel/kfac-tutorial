\switchcolumn[1]*
\codeblock{basics/reduction_factors}
\switchcolumn[0]

This tutorial is meant to be self-contained.
Therefore, we will start with an extensive introduction to KFAC-relevant concepts.
This allows us to build the code functionality we will later need to verify our implementation.

\paragraph{Roadmap.} First, we introduce the empirical risk (`the loss', \cref{subsec:empirical-risk-minimization}) whose curvature KFAC approximates, and neural networks (\cref{subsec:deep-neural-networks}).
One recurring theme in our discussion will be that the loss and neural net have probabilistic interpretations in most deep learning settings: Minimizing the empirical risk corresponds to maximum likelihood estimation where the neural net models a likelihood (\cref{subsec:probabilistic-interpretation}).
Next, since curvature information is based on the Hessian which contains second-order partial derivatives, we will talk about first- and second-order derivatives, and how to compute with them using PyTorch's automatic differentiation (\cref{subsec:derivatives}).
We conclude with an introduction to all curvature matrices relevant to our discussion (\cref{subsec:curvature-matrices}).
These include the Hessian, generalized Gauss-Newton (GGN) matrix and different flavours of the Fisher information matrix which follows from the probabilistic interpretation from \cref{subsec:probabilistic-interpretation}.

\subsection{Empirical Risk Minimization}\label{subsec:empirical-risk-minimization}
\input{sections/basics_empirical_risk.tex}

\switchcolumn[1]
\codeblock{basics/forward_pass}
\switchcolumn[0]
\subsection{Deep Neural Networks}\label{subsec:deep-neural-networks}
\input{sections/basics_neural_networks.tex}

\subsection{Probabilistic Interpretation}\label{subsec:probabilistic-interpretation}
\input{sections/basics_probabilistic_interpretation.tex}

\subsection{Derivatives \& Automatic Differentiation}\label{subsec:derivatives}
Let's talk about derivatives, which play a fundamental role in our goal to understand curvature matrices.
All of them are based on the Hessian, which emerges in a second-order Taylor expansion, and contains the second-order derivatives.
Here, we will build up to the Hessian, and how to compute with it using PyTorch's automatic differentiation.
For simplicity, we will fully rely on PyTorch's backpropagation and neglect other modes like forward-mode automatic differentiation.

\switchcolumn[0]
\subsubsection{Flattening}
\input{sections/basics_flattening.tex}

\switchcolumn[0]
\subsubsection{Jacobians, JVP, VJPs}
\input{sections/basics_jacobians.tex}

\switchcolumn[0]
\subsubsection{Hessians, HVPs}
\input{sections/basics_hessians.tex}

\switchcolumn[0]
\subsubsection{Partial Linearization, Generalized Gauss-Newtons (GGNs), GGNVPs}\label{sec:partial_linearization}
\input{sections/basics_linearization.tex}

\switchcolumn[0]
\subsection{Curvature Matrices in Deep Learning}\label{subsec:curvature-matrices}

We introduced Jacobians, Hessians, partial linearizations and the resulting generalized Gauss-Newton (GGN) in the language of automatic differentiation for arbitrary functions.
Let's switch gears and apply these concepts to deep learning.

\switchcolumn[1]*
\begin{figure}[!h]
  \centering
  \begin{minipage}[t]{0.495\linewidth}
    \centering
    $\cvec$\vspace{1ex}
    \includegraphics[width=\linewidth]{../kfs/plots/synthetic_cvec_hessian.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.495\linewidth}
    \centering
    $\rvec$\vspace{1ex}
    \includegraphics[width=\linewidth]{../kfs/plots/synthetic_rvec_hessian.pdf}
  \end{minipage}
  \\
  \begin{minipage}[t]{0.495\linewidth}
    \centering
    \includegraphics[width=\linewidth]{../kfs/plots/synthetic_cvec_hessian_bda.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.495\linewidth}
    \centering
    \includegraphics[width=\linewidth]{../kfs/plots/synthetic_rvec_hessian_bda.pdf}
  \end{minipage}
  \caption{\textbf{The Hessian of a neural net exhibits block structure.}
    We visualize the Hessian and its block-diagonal approximation using different flattening schemes.
    We use synthetic data ($N=100$) on an MLP with three fully-connected layers (5-4-4-3) and ReLU activations and square loss.
    Blocks correspond to second-order derivatives \wrt weights and biases of the different layers, yielding a $6 \times 6$ block structure.
    Hessian blocks are visually highlighted with white lines.
    The left column uses $\cvec$-flattening, the right column $\rvec$-flattening.
    Plots produced with \repofile{plots/synthetic_hessian}.
  }\label{fig:hessian-block-structure}
\end{figure}
\switchcolumn[0]

\paragraph{Parameter list/tuple format.} One new aspect we have to deal with is that ML libraries like PyTorch represent parameters as lists/tuples of variables, each of which are associated to a different layer.
This layer structure in the parameters gives rise to a block structure in the Jacobians, Hessians (see \Cref{fig:hessian-block-structure} for an example), and GGNs.
In the context of KFAC, we will focus on specific blocks.

Typically, a neural net $f(\vtheta, \vx): \Theta \times \gX \to \gF$ has parameters in list/tuple-format,
\begin{align*}
  \vtheta = (\vtheta^{(1)}, \vtheta^{(2)}, \ldots, \vtheta^{(L)}),
\end{align*}
where each $\vtheta^{(l)}$ is an arbitrary parameter tensor.
To be able to use matrix expressions, we will often consider the concatenation of flattened parameters,
\begin{align*}
  \vec(\vtheta)
  =
  \begin{pmatrix}
    \vec(\vtheta^{(1)}) \\
    \vec(\vtheta^{(2)}) \\
    \vdots              \\
    \vec(\vtheta^{(L)})
  \end{pmatrix}
  \in \sR^D
  \,,
\end{align*}
with $\vec \in \{ \rvec, \cvec \}$ one of the previously described flattening operations (\Cref{def:cvec,def:rvec}).
This convention generalizes $\vec$ to handle parameters in list/tuple format by applying the original definition to each element and concatenating the results.
In code we will still work with the list/tuple format.

\paragraph{Recap: Empirical risk and shorthands.}
We consider a data set $\sD = \{(\vx_n, \vy_n) \in \gX \times \gY \mid n = 1, \dots, N \}$ containing $N$ independent and identically distributed (\iid) samples.
The inputs are processed by a neural net $f: \Theta \times \gX \to \gF$ and their predictions are scored with a criterion function $c: \gF \times \gY \to \sR$, like square loss (\Cref{ex:square_loss}) or softmax cross-entropy loss (\Cref{ex:cross_entropy_loss}).
For each datum $n$, we define its prediction function $\vf_n: \Theta \to \gF, \vtheta \mapsto \vf_n(\vtheta) = f(\vtheta, \vx_n)$ and its criterion function $c_n: \gF \to \sR, \vf \mapsto c(\vf, \vy_n)$.
Combining both, we obtain the per-datum loss function (\wrt the net's parameters),
\begin{align*}
  (\ell_n: \Theta \to \sR) = (c_n \circ \vf_n: \Theta \to \gF \to \sR)
\end{align*}
We will often use the shorthands $c_n, \vf_n, \ell_n$ for the function values (rather than functions) of the per-datum criteria, predictions, and losses.

We accumulate the per-datum losses $\{\ell_1, \dots, \ell_N\}$ into a single scalar, which yields the empirical risk
\begin{align*}
  \gL_{\sD}: \Theta \to \sR,
  \qquad
  \vtheta \mapsto \gL_{\sD}(\vtheta) = R \sum_{n=1}^N \ell_n(\vtheta)\,,
\end{align*}
where $R$ is the reduction factor (see \Cref{eq:empirical_risk} for details).
$\sD$ can be any collection of data points, \eg the full data set or a mini-batch.

\subsubsection{The Hessian}\label{sec:basics_dl_hessian}
\input{sections/basics_dl_hessian.tex}

\subsubsection{The Generalized Gauss-Newton}
\input{sections/basics_dl_ggn.tex}

\subsubsection{The Fisher}\label{sec:fisher}
\input{sections/basics_dl_fisher.tex}

\subsubsection{Connection between GGN \& Fisher}\label{subsec:connection-ggn-fisher}
The GGN, type-I, and type-II Fisher are all weighted sums of matrices sandwiched between the per-sample Jacobians (remember that we can mentally set $-\log r(\rvy \mid \vf_n) = c(\vf_n, \rvy)$ for square and softmax cross-entropy loss):
\begin{align*}
  \mG(\vtheta)
  & =
    R \sum_n
    \begin{aligned}[t]
      & (\jac_{\vtheta}\vf_n)^\top                           \\
      & \textcolor{VectorBlue}{\hess_{\vf_n}c(\vf_n, \vy_n)} \\
      & \jac_{\vtheta}\vf_n\,,
    \end{aligned}
  \\
  \mF^{\text{II}}(\vtheta)
  & =
    R \sum_n
    \begin{aligned}[t]
      & (\jac_{\vtheta}\vf_n)^\top                                                                 \\
      & \textcolor{VectorPink}{\E_{r(\rvy \mid \vf_n)}[-\hess_{\vf_n} \log( r(\rvy \mid \vf_n)) ]} \\
      & \jac_{\vtheta}\vf_n\,,
    \end{aligned}
  \\
  \mF^{\text{I}}(\vtheta)
  & =
    R \sum_n
    \begin{aligned}[t]
      & (\jac_{\vtheta}\vf_n)^\top
      \\
      & \colored[VectorTeal]{\E_{r(\rvy \mid \vf_n)}[}
      \\
      &\begin{aligned}[t]
        &\quad \colored[VectorTeal]{-\nabla_{\vf_n} \log( r(\rvy \mid \vf_n))}
        \\
        &\quad \colored[VectorTeal]{(-\nabla_{\vf_n} \log( r(\rvy \mid \vf_n)))^{\top}]}
      \end{aligned}
      \\
      & \jac_{\vtheta}\vf_n\,.
    \end{aligned}
\end{align*}
In previous sections, we showed that for square loss and softmax cross-entropy the criterion function's Hessian $\hess_\vf c(\vf, \vy) = -\hess_{\vf} \log( r(\rvy = \vy \mid \vf)$ does not depend on the value of the target random variable $\rvy$!
Therefore, the expectation in the type-II Fisher effectively disappears and we are free to set $\rvy = \vy_n$ because this does not change the Hessian.
This means the type-II Fisher and GGN are equivalent for these losses.
Note that we cannot drop the expectation in the type-I Fisher, though.\footnote{This is precisely why we needed a separate definition for the Monte Carlo-approximated type-I Fisher,
  as it yields different values from the type-II Fisher even for the square and softmax cross-entropy losses.}
But from the equivalence of type-I and type-II Fisher, we know that it also equals the GGN in the above scenarios.

\subsubsection{The Empirical Fisher (EF)}\label{sec:emp_fisher}
\input{sections/basics_dl_emp_fisher.tex}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
